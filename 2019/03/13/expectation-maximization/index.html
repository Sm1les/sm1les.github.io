<!doctype html>
<html lang="en">
    <head><meta name="generator" content="Hexo 3.8.0">
		
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta http-equiv="Content-Type" content="text/html" charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="description" content>
        <link rel="shortcut icon" href="/styles/images/logo.jpg">
        <link rel="alternate" type="application/rss+xml" title="Sm1les" href="/atom.xml">

        <title>EM算法 | Sm1les&#39;s blog</title>
        <meta name="description" content="{{meta_description}}">

        <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="/styles/crisp.css">
        <meta name="HandheldFriendly" content="True">
        <meta name="MobileOptimized" content="320">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    </head>
    
		<body class="post-template">
	

        <!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="header">
            <a id="logo" href="/"><img src="/styles/images/logo.jpg" alt="Sm1les's blog"></a>
            <h1><a href="/">Sm1les</a></h1>
            <p></p>
            <div id="follow-icons">
	<a href="#"><i class="fa fa-instagram fa-2x"></i></a>
	<a href="#"><i class="fa fa-pinterest-square fa-2x"></i></a>
	<a href="http://github.com/Sm1les"><i class="fa fa-github-square fa-2x"></i></a>
	<a href="#"><i class="fa fa-facebook-square fa-2x"></i></a>
	<a href="#"><i class="fa fa-twitter-square fa-2x"></i></a>
	<a href="#"><i class="fa fa-linkedin-square fa-2x"></i></a>
	<a href="/atom.xml"><i class="fa fa-rss-square fa-2x"></i></a>
</div>
<div id="tag-cloud">

        <a href="/tags/EM算法/" style="font-size: 10px;">EM算法</a> <a href="/tags/KKT条件/" style="font-size: 10px;">KKT条件</a> <a href="/tags/Logistic回归/" style="font-size: 20px;">Logistic回归</a> <a href="/tags/Slater条件/" style="font-size: 10px;">Slater条件</a> <a href="/tags/对偶/" style="font-size: 10px;">对偶</a> <a href="/tags/广义线性模型/" style="font-size: 10px;">广义线性模型</a> <a href="/tags/指数族分布/" style="font-size: 10px;">指数族分布</a> <a href="/tags/损失函数/" style="font-size: 10px;">损失函数</a> <a href="/tags/最大熵/" style="font-size: 20px;">最大熵</a> <a href="/tags/梯度下降法/" style="font-size: 10px;">梯度下降法</a> <a href="/tags/正则化/" style="font-size: 10px;">正则化</a> <a href="/tags/牛顿法/" style="font-size: 10px;">牛顿法</a> <a href="/tags/线性回归/" style="font-size: 10px;">线性回归</a>
    
</div>
<h6><a href="/about">About</a></h6>

        </header>

        <main id="content">
        

<article class="post">
  
  三月 13, 2019
  
    <span class="taglist">  &middot; 
    
    
      <a href="/tags/EM算法/">EM算法</a> 
    
    </span>
  

  <h1 class="post-title">EM算法</h1>
  <section class="post-content article-entry">
    <p>在推导EM算法之前先学习一下<strong>Jensen不等式<sup><a href="#ref1">[1]</a></sup></strong>，Jensen不等式的定义如下：若<br>$f$是<strong>凸函数</strong>，则</p>
<script type="math/tex; mode=display">f(t x_1 + (1-t)x_2)\leq tf(x_1)+(1-t)f(x_2)</script><p>其中$t\in [0,1]$。同理，若$f$是<strong>凹函数</strong>，则只需将上式的$\leq$换成$\geq$即可。</p>
<p>将上式中的$t$推广到$n$个同样成立，即</p>
<script type="math/tex; mode=display">f(t_1 x_1 + t_2x_2+...+t_nx_n)\leq t_1f(x_1)+t_2f(x_2)+...+t_nf(t_n)</script><p>其中$t_1,t_2,…,t_n\in[0,1],t_1+t_2+…+t_n=1$。</p>
<p>上式在概率论中通常以如下形式出现：</p>
<script type="math/tex; mode=display">\varphi(E[X])\leq E[\varphi(X)]</script><p>其中$X$是一个随机变量，$\varphi$为凸函数，$E[X]$为随机变量$X$的期望。</p>
<h3 id="EM（Expectation-Maximization）算法的导出-2"><a href="#EM（Expectation-Maximization）算法的导出-2" class="headerlink" title="EM（Expectation Maximization）算法的导出[2]"></a>EM（Expectation Maximization）算法的导出<sup><a href="#ref2">[2]</a></sup></h3><p>我们面对一个含有隐变量的概率模型，目标是极大化观测数据（不完全数据）$Y$关于参数$\theta$的对数似然函数，即极大化</p>
<script type="math/tex; mode=display">L(\theta)=\ln P(Y\vert \theta)=\ln \sum_Z P(Y,Z\vert\theta)=\ln \left(\sum_Z P(Y\vert Z,\theta)P(Z\vert \theta)\right)</script><p>注意到这一极大化的主要困难是上式中有未观测数据$Z$并有包含和（$Z$为离散型时）或者积分（$Z$为连续型时）的对数。EM算法采用的是通过迭代逐步近似极大化$L(\theta)$：假设在第$i$次迭代后$\theta$的估计值是$\theta^{(i)}$，我们希望新的估计值$\theta$能使$L(\theta)$增加，即$L(\theta)&gt;L(\theta^{(i)})$，并逐步达到极大值。为此，考虑两者的差：</p>
<script type="math/tex; mode=display">\begin{aligned}
L(\theta)-L(\theta^{(i)})&=\ln \left(\sum_Z P(Y\vert Z,\theta)P(Z\vert \theta)\right)-\ln P(Y\vert\theta^{(i)}) \\
&=\ln \left(\sum_Z P(Z\vert Y,\theta^{(i)}) \cfrac{P(Y\vert Z,\theta)P(Z\vert \theta)}{P(Z\vert Y,\theta^{(i)})}\right)-\ln P(Y\vert\theta^{(i)})
\end{aligned}</script><p>注意，上式中采用的是和参考文献<a href="#ref3">[3]</a>一样上下同时乘以$P(Z\vert Y,\theta^{(i)})$，而参考文献<a href="#ref2">[2]</a>中一开始上下同乘的是$P(Y\vert Z,\theta^{(i)})$，而后续却用的是$P(Z\vert Y,\theta^{(i)})$，不知是另有原由还是作者笔误，暂不知晓，接着由上述Jensen不等式以及$\ln$函数为<strong>凹函数</strong>可得：</p>
<script type="math/tex; mode=display">\begin{aligned}
L(\theta)-L(\theta^{(i)})&=\ln \left(\sum_Z P(Z\vert Y,\theta^{(i)}) \cfrac{P(Y\vert Z,\theta)P(Z\vert \theta)}{P(Z\vert Y,\theta^{(i)})}\right)-\ln P(Y\vert\theta^{(i)})  \\
&\geq \sum_Z P(Z\vert Y,\theta^{(i)})\ln \cfrac{P(Y\vert Z,\theta)P(Z\vert \theta)}{P(Z\vert Y,\theta^{(i)})}-\ln P(Y\vert\theta^{(i)}) \\
&= \sum_Z P(Z\vert Y,\theta^{(i)})\ln \cfrac{P(Y\vert Z,\theta)P(Z\vert \theta)}{P(Z\vert Y,\theta^{(i)})}-1\cdot \ln P(Y\vert\theta^{(i)}) \\
&= \sum_Z P(Z\vert Y,\theta^{(i)})\ln \cfrac{P(Y\vert Z,\theta)P(Z\vert \theta)}{P(Z\vert Y,\theta^{(i)})}-\sum_Z P(Z\vert Y,\theta^{(i)})\cdot \ln P(Y\vert\theta^{(i)}) \\
&=\sum_Z P(Z\vert Y,\theta^{(i)}) \left( \ln \cfrac{P(Y\vert Z,\theta)P(Z\vert \theta)}{P(Z\vert Y,\theta^{(i)})} - \ln P(Y\vert\theta^{(i)}) \right)\\
&= \sum_Z P(Z\vert Y,\theta^{(i)})\ln \cfrac{P(Y\vert Z,\theta)P(Z\vert \theta)}{P(Z\vert Y,\theta^{(i)})P(Y\vert\theta^{(i)})}
\end{aligned}</script><p>令</p>
<script type="math/tex; mode=display">B(\theta,\theta^{(i)})=L(\theta^{(i)})+\sum_Z P(Z\vert Y,\theta^{(i)})\ln \cfrac{P(Y\vert Z,\theta)P(Z\vert \theta)}{P(Z\vert Y,\theta^{(i)})P(Y\vert\theta^{(i)})}</script><p>则</p>
<script type="math/tex; mode=display">L(\theta)\geq B(\theta,\theta^{(i)})</script><p>即函数$B(\theta,\theta^{(i)})$是$L(\theta)$的一个下界，此时若设$\theta^{(i+1)}$使得$B(\theta,\theta^{(i)})$达到极大，也即</p>
<script type="math/tex; mode=display">B(\theta^{(i+1)},\theta^{(i)}) \geq B(\theta^{(i)},\theta^{(i)})</script><p>由于$L(\theta^{(i)})=B(\theta^{(i)},\theta^{(i)})$，所以可以进一步推得：</p>
<script type="math/tex; mode=display">L(\theta^{(i+1)})\geq B(\theta^{(i+1)},\theta^{(i)})\geq B(\theta^{(i)},\theta^{(i)})=L(\theta^{(i)})</script><script type="math/tex; mode=display">L(\theta^{(i+1)})\geq L(\theta^{(i)})</script><p><center>
<img src="./em.png" width="60%" height="60%"><br>
此图是对上述不等式的直观解释
</center><br>因此，任何可以使$B(\theta,\theta^{(i)})$增大的$\theta$，也可以使$L(\theta)$增大，于是问题就转化为了求解能使得$B(\theta,\theta^{(i)})$达到极大的$\theta^{(i+1)}$，即</p>
<script type="math/tex; mode=display">\begin{aligned}
\theta^{(i+1)}&=\mathop{\arg\max}_{\theta}B(\theta,\theta^{(i)}) \\
&=\mathop{\arg\max}_{\theta}\left( L(\theta^{(i)})+\sum_Z P(Z\vert Y,\theta^{(i)})\ln \cfrac{P(Y\vert Z,\theta)P(Z\vert \theta)}{P(Z\vert Y,\theta^{(i)})P(Y\vert\theta^{(i)})}\right)
\end{aligned}</script><p>略去对$\theta$的极大化而言是常数的项：</p>
<script type="math/tex; mode=display">\begin{aligned}
\theta^{(i+1)}&=\mathop{\arg\max}_{\theta}\left(\sum_Z P(Z\vert Y,\theta^{(i)})\ln\left( P(Y\vert Z,\theta)P(Z\vert \theta)\right)\right) \\
&=\mathop{\arg\max}_{\theta}\left(\sum_Z P(Z\vert Y,\theta^{(i)})\ln P(Y,Z\vert \theta)\right) \\
&=\mathop{\arg\max}_{\theta}Q(\theta,\theta^{(i)})
\end{aligned}</script><p>到此即完成了EM算法的一次迭代，求出的$\theta^{(i+1)}$作为下一次迭代的初始$\theta^{(i)}$。综上可以总结出EM算法的“E步”和“M步”分别为：<br><strong>E步：</strong>计算完全数据的对数似然函数$\ln P(Y,Z\vert \theta)$关于在给定观测数据$Y$和当前参数$\theta^{(i)}$下对未观测数据$Z$的条件概率分布$ P(Z\vert Y,\theta^{(i)})$的期望，也即$Q$函数：</p>
<script type="math/tex; mode=display">Q(\theta,\theta^{(i)})=E_Z[\ln P(Y,Z\vert \theta)\vert Y,\theta^{(i)}]=\sum_Z P(Z\vert Y,\theta^{(i)})\ln P(Y,Z\vert \theta)</script><p><strong>M步：</strong>求使得$Q$函数到达极大的$\theta^{(i+1)}$。<br>EM算法的基本思想可以总结为<sup><a href="#ref4">[4]</a></sup>：若参数$\theta$已知，则可根据观测数据$Y$推断出最优隐变量$Z$的值（E步）；反之，若$Z$的值已知，则可方便地对参数$\theta$做极大似然估计（M步）。</p>
<h3 id="EM算法在高斯混合模型中的应用-2-3-5"><a href="#EM算法在高斯混合模型中的应用-2-3-5" class="headerlink" title="EM算法在高斯混合模型中的应用[2][3][5]"></a>EM算法在高斯混合模型中的应用<sup><a href="#ref2">[2]</a></sup><sup><a href="#ref3">[3]</a></sup><sup><a href="#ref5">[5]</a></sup></h3><p>高斯混合模型（Gaussian Mixture Model，GMM）定义如下：</p>
<script type="math/tex; mode=display">P(y|\theta)=\sum_{k=1}^{K}\alpha_k\phi(y\vert \theta_k)</script><p>其中$\alpha_k$是系数，$\alpha_k\geq0,\sum_{k=1}^{K}\alpha_k=1$，$\theta_k=(\mu_k,\sigma_k^2)$，$\phi(y\vert \theta_k)$为高斯分布密度：</p>
<script type="math/tex; mode=display">\phi(y\vert \theta_k) =\cfrac{1}{\sqrt{2\pi}\sigma_k}\exp\left(-\cfrac{(y-\mu_k)^2}{2\sigma_k^2}\right)</script><p>假设观测数据$y_1,y_2,…,y_N$由上述高斯混合模型生成，现用EM算法来估计其参数$\theta=(\alpha_1,\alpha_2,…,\alpha_K;\theta_1,\theta_2,…,\theta_K)$。</p>
<h5 id="1-明确隐变量，写出完全数据的对数似然函数"><a href="#1-明确隐变量，写出完全数据的对数似然函数" class="headerlink" title="1.明确隐变量，写出完全数据的对数似然函数"></a>1.明确隐变量，写出完全数据的对数似然函数</h5><p>假设观测数据$y_j$是这样产生的：首先依概率$\alpha_k$选择第$k$个高斯分布分模型$\phi(y\vert \theta_k)$，接着由该分模型生成观测数据$y_j$，此时观测数据$y_j$（其中$j=1,2,…,N$）是已知的，反映观测数据$y_j$来自第$k$个分模型的数据是未知的，我们用隐变量$z_{jk}$来表示，其定义如下：</p>
<script type="math/tex; mode=display">
z_{jk} = \left\{ \begin{array}{rcl}
1 & 第j个观测数据y_j来自第k个分模型 \\ 
 0 & 否则 \\
\end{array}\right.</script><p>所以$z_{jk}$是一个0-1随机变量。确定了未观测数据的表示形式后便可以写出完全数据的对数似然函数：</p>
<script type="math/tex; mode=display">\begin{aligned}
\ln P(Y,Z\vert \theta)&=\ln\left( P(Y\vert Z,\theta)P(Z\vert \theta)\right) \\
&=\ln\left( \prod_{j=1}^{N}\left( \prod_{k=1}^{K} [\phi(y_j\vert\theta_k)]^{z_{jk}}\times \prod_{k=1}^{K} \alpha_{k}^{z_{jk}}\right)\right) \\
&=\ln\left( \prod_{j=1}^{N}\left( \prod_{k=1}^{K} [\phi(y_j\vert\theta_k)]^{z_{jk}} \alpha_{k}^{z_{jk}}\right)\right) \\
&=\ln\left( \prod_{j=1}^{N}\prod_{k=1}^{K}[\alpha_{k}\phi(y_j\vert\theta_k)]^{z_{jk}}\right) \\
&= \sum_{j=1}^{N}\sum_{k=1}^{K}\ln\left([\alpha_{k}\phi(y_j\vert\theta_k)]^{z_{jk}}\right) \\
&= \sum_{j=1}^{N}\sum_{k=1}^{K}{z_{jk}}\ln(\alpha_{k}\phi(y_j\vert\theta_k)) \\
&= \sum_{j=1}^{N}\sum_{k=1}^{K}{z_{jk}}(\ln\alpha_{k}+\ln\phi(y_j\vert\theta_k)) \\
\end{aligned}</script><h5 id="2-EM算法E步：确定-Q-函数"><a href="#2-EM算法E步：确定-Q-函数" class="headerlink" title="2.EM算法E步：确定$Q$函数"></a>2.EM算法E步：确定$Q$函数</h5><script type="math/tex; mode=display">\begin{aligned}
Q(\theta,\theta^{(i)})&=\sum_Z P(Z\vert Y,\theta^{(i)})\ln P(Y,Z\vert \theta) \\
&=\sum_Z P(Z\vert Y,\theta^{(i)})\left\{\sum_{j=1}^{N}\sum_{k=1}^{K}{z_{jk}}(\ln\alpha_{k}+\ln\phi(y_j\vert\theta_k))\right\}\\
&=\sum_{j=1}^{N}\sum_{k=1}^{K}\left\{\sum_{z_{jk}}P(z_{jk}\vert y_j,\theta^{(i)}){z_{jk}}(\ln\alpha_{k}+\ln\phi(y_j\vert\theta_k))\right\} \quad(此处\sum_{z_{jk}}表示遍历z_{jk}的两种取值情况：0和1)\\
&=\sum_{j=1}^{N}\sum_{k=1}^{K}\left\{P(z_{jk}=1\vert y_j,\theta^{(i)})\cdot 1 \cdot (\ln\alpha_{k}+\ln\phi(y_j\vert\theta_k))+P(z_{jk}=0\vert y_j,\theta^{(i)})\cdot 0 \cdot (\ln\alpha_{k}+\ln\phi(y_j\vert\theta_k))\right\} \\
&=\sum_{j=1}^{N}\sum_{k=1}^{K}\left\{P(z_{jk}=1\vert y_j,\theta^{(i)})(\ln\alpha_{k}+\ln\phi(y_j\vert\theta_k))\right\}\\    
\end{aligned}</script><p>其中$P(z_{jk}=1\vert y_j,\theta^{(i)})$表示在给定$\theta=\theta^{(i)}$的条件下，某个观测数据$y_j$是由第$k$个分模型生成的概率，称为分模型$k$对观测数据$y_j$的响应度。由<strong>贝叶斯定理</strong>可进一步推得<sup><a href="#ref4">[4]</a></sup>：</p>
<script type="math/tex; mode=display">\begin{aligned}
P(z_{jk}=1\vert y_j,\theta^{(i)})&=\cfrac{P(y_j\vert z_{jk}=1,\theta^{(i)})\cdot P(z_{jk}=1,\theta^{(i)})}{P(y_j,\theta^{(i)})} \\
&=\cfrac{P(y_j\vert z_{jk}=1,\theta^{(i)})\cdot P(z_{jk}=1,\theta^{(i)})}{\sum_{l=1}^{K}\alpha_l\phi(y_j\vert \theta_l^{(i)})} \\
&=\cfrac{\phi(y_j\vert \theta_k^{(i)})\cdot \alpha_k}{\sum_{l=1}^{K}\alpha_l\phi(y_j\vert \theta_l^{(i)})} \\
&=\cfrac{\alpha_k\cdot \phi(y_j\vert \theta_k^{(i)})}{\sum_{l=1}^{K}\alpha_l\phi(y_j\vert \theta_l^{(i)})}
\end{aligned}</script><p>$P(z_{jk}=1\vert y_j,\theta^{(i)})$也可以看做是<strong>某个</strong>$z_{jk}$的期望<strong>（注意不是所有的，因为$z_{jk}$一共有$N\times K$个，每个$z_{jk}$均为一个独立的随机变量，此处说的只是其中某一个，例如$z_{12}$的期望值）</strong>，证明如下：由于每个$z_{jk}$均为$0-1$随机变量，所以对于某个$z_{jk}$来说，其期望为：</p>
<script type="math/tex; mode=display">\begin{aligned}
E[z_{jk}]&=\sum_{z_{jk}}\left(P(z_{jk}\vert y_j,\theta^{(i)})\cdot z_{jk}\right) \\
&=P(z_{jk}=1\vert y_j,\theta^{(i)})\cdot 1 + P(z_{jk}=0\vert y_j,\theta^{(i)})\cdot 0\\
&=P(z_{jk}=1\vert y_j,\theta^{(i)})\\
\end{aligned}</script><p>证毕。现将$P(z_{jk}=1\vert y_j,\theta^{(i)})$简记为$\gamma_{jk}$，则：</p>
<script type="math/tex; mode=display">\begin{aligned}
Q(\theta,\theta^{(i)})&=\sum_{j=1}^{N}\sum_{k=1}^{K}\left\{\gamma_{jk}(\ln\alpha_{k}+\ln\phi(y_j\vert\theta_k))\right\} \\
&=\sum_{j=1}^{N}\sum_{k=1}^{K}\left\{\gamma_{jk}\ln\alpha_{k}+\gamma_{jk}\ln\phi(y_j\vert\theta_k)\right\} \\
&=\sum_{j=1}^{N}\sum_{k=1}^{K}\left\{\gamma_{jk}\ln\alpha_{k}+\gamma_{jk}\ln\left(\cfrac{1}{\sqrt{2\pi}\sigma_k}\exp\left(-\cfrac{(y_j-\mu_k)^2}{2\sigma_k^2}\right)\right)\right\} \\
&=\sum_{j=1}^{N}\sum_{k=1}^{K}\left\{\gamma_{jk}\ln\alpha_{k}+\gamma_{jk}\left(\ln\cfrac{1}{\sqrt{2\pi}}-\ln \sigma_k-\cfrac{(y_j-\mu_k)^2}{2\sigma_k^2}\right)\right\} \\
&=\sum_{j=1}^{N}\sum_{k=1}^{K}\left\{\gamma_{jk}\ln\alpha_{k}+\gamma_{jk}\ln\cfrac{1}{\sqrt{2\pi}}-\gamma_{jk}\ln \sigma_k-\gamma_{jk}\cfrac{(y_j-\mu_k)^2}{2\sigma_k^2}\right\}
\end{aligned}</script><h5 id="3-EM算法M步：求使得-Q-函数达到极大的-theta-i-1-alpha-1-i-1-alpha-2-i-1-…-alpha-K-i-1-theta-1-i-1-theta-2-i-1-…-theta-K-i-1"><a href="#3-EM算法M步：求使得-Q-函数达到极大的-theta-i-1-alpha-1-i-1-alpha-2-i-1-…-alpha-K-i-1-theta-1-i-1-theta-2-i-1-…-theta-K-i-1" class="headerlink" title="3.EM算法M步：求使得$Q$函数达到极大的$\theta^{(i+1)}=(\alpha_1^{(i+1)},\alpha_2^{(i+1)},…,\alpha_K^{(i+1)};\theta_1^{(i+1)},\theta_2^{(i+1)},…,\theta_K^{(i+1)})$"></a>3.EM算法M步：求使得$Q$函数达到极大的$\theta^{(i+1)}=(\alpha_1^{(i+1)},\alpha_2^{(i+1)},…,\alpha_K^{(i+1)};\theta_1^{(i+1)},\theta_2^{(i+1)},…,\theta_K^{(i+1)})$</h5><p>$Q(\theta,\theta^{(i)})$关于$\mu_k$求偏导（此时$k$是固定的）：</p>
<script type="math/tex; mode=display">\begin{aligned}
\cfrac{\partial{Q(\theta,\theta^{(i)})}}{\partial{\mu_k}}&=\sum_{j=1}^{N}\left\{0+0-0-\cfrac{\gamma_{jk}}{2\sigma_k^2}\cdot 2 \cdot(y_j-\mu_k)\right\} \\
&=\sum_{j=1}^{N}\left\{-\cfrac{\gamma_{jk}}{\sigma_k^2}\cdot(y_j-\mu_k)\right\} \\
&=\cfrac{\mu_k}{\sigma_k^2}\sum_{j=1}^{N}\gamma_{jk}-\cfrac{1}{\sigma_k^2}\sum_{j=1}^{N}\gamma_{jk}y_j \\
\end{aligned}</script><p>令上式等于0即可得：</p>
<script type="math/tex; mode=display">\mu_k^{(i+1)}=\cfrac{\sum_{j=1}^{N}\gamma_{jk}y_j}{\sum_{j=1}^{N}\gamma_{jk}}</script><p>$Q(\theta,\theta^{(i)})$关于$\sigma_k$求偏导（此时$k$是固定的）：</p>
<script type="math/tex; mode=display">\begin{aligned}
\cfrac{\partial{Q(\theta,\theta^{(i)})}}{\partial{\sigma_k}}&=\sum_{j=1}^{N}\left\{0+0-\gamma_{jk}\sigma_k^{-1}-\gamma_{jk}(y_j-\mu_k)^2\cdot\cfrac{1}{2}\cdot-2\cdot\sigma_k^{-3} \right\} \\
&=\sum_{j=1}^{N}\left\{\gamma_{jk}(y_j-\mu_k)^2\sigma_k^{-3}-\gamma_{jk}\sigma_k^{-1} \right\} \\
&=\sigma_k^{-3}\sum_{j=1}^{N}\gamma_{jk}(y_j-\mu_k)^2-\sigma_k^{-1}\sum_{j=1}^{N}\gamma_{jk} \\
\end{aligned}</script><p>令上式等于0即可得：</p>
<script type="math/tex; mode=display">[\sigma_k^{(i+1)}]^{2}=\cfrac{\sum_{j=1}^{N}\gamma_{jk}(y_j-\mu_k)^2}{\sum_{j=1}^{N}\gamma_{jk}}</script><p>求$\alpha_k^{(i+1)}$时需要注意，此时存在约束$\sum_{k=1}^{K}\alpha_k^{(i+1)}=1$，因此考虑使用拉格朗日乘子法，首先构造拉格朗日函数：</p>
<script type="math/tex; mode=display">L(\theta,\beta)=Q(\theta,\theta^{(i)})+\beta(\sum_{k=1}^{K}\alpha_k-1)</script><p>对$L(\theta,\beta)$关于$\sigma_k$求偏导（此时$k$是固定的）：</p>
<script type="math/tex; mode=display">\begin{aligned}
\cfrac{\partial{L(\theta,\beta)}}{\partial{\alpha_k}}&=\cfrac{\sum_{j=1}^{N}\gamma_{jk}}{\alpha_{k}}+\beta \\
\end{aligned}</script><p>令上式等于0即可得：</p>
<script type="math/tex; mode=display">\alpha_{k}^{(i+1)}=\cfrac{\sum_{j=1}^{N}\gamma_{jk}}{-\beta}</script><p>又由约束条件可得：</p>
<script type="math/tex; mode=display">\sum_{k=1}^{K}\alpha_k^{(i+1)}=\cfrac{\sum_{k=1}^{K}\sum_{j=1}^{N}\gamma_{jk}}{-\beta}=1</script><p>其中$\sum_{k=1}^{K}\sum_{j=1}^{N}\gamma_{jk}=\sum_{j=1}^{N}\sum_{k=1}^{K}\gamma_{jk}=\sum_{j=1}^{N}1=N$，所以</p>
<script type="math/tex; mode=display">\cfrac{N}{-\beta}=1\Rightarrow -\beta=N\Rightarrow \alpha_{k}^{(i+1)}=\cfrac{\sum_{j=1}^{N}\gamma_{jk}}{N}</script><p>【注】：</p>
<ul>
<li>上述“M步”中求极大化时直接就求偏导令其等于0进行求解，求出来的解一定是极大值点吗？不会是极小值点吗？不应该先做凹凸性判断吗？但是做凹凸性判断时$\sigma_k$的二阶偏导正负性无法判断呀。。。难道另有隐情？为何参考文献里都没提？挖坑待填。。。</li>
<li><a href="#ref2">[2]</a>中三硬币模型使用EM算法的推导过程参见<a href="#ref2">[6]</a></li>
</ul>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p><span id="ref1">[1] <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality" target="_blank" rel="noopener">Jensen’s inequality</a></span><br><span id="ref2">[2] 李航.《统计学习方法》</span><br><span id="ref3">[3] cs229-notes8</span><br><span id="ref4">[4] 周志华.《机器学习》</span><br><span id="ref5">[5] <a href="http://alexkong.net/2014/07/GMM-and-EM/" target="_blank" rel="noopener">高斯混合模型和期望最大化算法</a></span><br><span id="ref6">[6] <a href="http://www.cnblogs.com/Determined22/p/5776791.html" target="_blank" rel="noopener">NLP —— 图模型（零）：EM算法简述及简单示例（三硬币模型）</a></span></p>

  </section>
  <footer class="post-footer">
    <!--
    <section class="author">
      <h4>Sm1les</h4>
      <p></p>
    </section>
    -->
  </footer>
</article>

<nav class="pagination" role="pagination">
    
    <span class="page-number">•</span>
    
    <a class="older-posts" href="/2019/03/01/gradient-descent-and-newton-method/">
        <!--梯度下降法与牛顿法--> next →
    </a>
    
</nav>

<!-- Begin Comments Code -->

  <div id="comments">
    <!-- 多说评论框 start -->
    <div class="ds-thread" data-thread-key="post-expectation-maximization" data-title="EM算法" data-url="http://www.sm1les.com/2019/03/13/expectation-maximization/"></div>
    <!-- 多说评论框 end -->
    <!-- 多说公共JS代码 start (一个网页只需插入一次) -->
    <script type="text/javascript">
    var duoshuoQuery = {short_name:'sm1les'};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] 
         || document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
      </script>
    <!-- 多说公共JS代码 end -->
  </div>

<!-- End Comments Code -->


        </main>
        <footer id="footer">
            <section id="footer-message">&copy; 2019 Sm1les. All rights reserved. Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. <a href="https://github.com/guolin/crisp-hexo-theme" target="_blank">crisp</a> theme by <a href="http://guolin.github.io" target="_blank">Guo Lin</a>.</section>
        </footer>
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','/js/ga.js','ga');
            ga('create', 'UA-131477813-1', 'auto');
            ga('send', 'pageview');
        </script>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>


