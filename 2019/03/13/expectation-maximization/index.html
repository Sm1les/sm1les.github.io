<!DOCTYPE html>
<html lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>EM算法的两种推导方式 | Sm1les&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="EM算法有两种常见的推导方式，一种是李航老师在《统计学习方法》里面给出的，另一种是吴恩达老师在CS229课上讲到的，两种推导方式各有千秋，但都殊途同归，下面对这两种推导方式进行一个总结。由于两种推导方式里都用到了一个经典的不等式——Jensen不等式[1]，所以下面先铺垫一下Jensen不等式。Jensen不等式的定义如下：若$f$是凸函数，则 f\left(t x_1 + (1-t)x_2\ri">
<meta name="keywords" content="EM算法,Jensen不等式">
<meta property="og:type" content="article">
<meta property="og:title" content="EM算法的两种推导方式">
<meta property="og:url" content="http://sm1les.com/2019/03/13/expectation-maximization/index.html">
<meta property="og:site_name" content="Sm1les&#39;s blog">
<meta property="og:description" content="EM算法有两种常见的推导方式，一种是李航老师在《统计学习方法》里面给出的，另一种是吴恩达老师在CS229课上讲到的，两种推导方式各有千秋，但都殊途同归，下面对这两种推导方式进行一个总结。由于两种推导方式里都用到了一个经典的不等式——Jensen不等式[1]，所以下面先铺垫一下Jensen不等式。Jensen不等式的定义如下：若$f$是凸函数，则 f\left(t x_1 + (1-t)x_2\ri">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-10-16T05:25:56.919Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="EM算法的两种推导方式">
<meta name="twitter:description" content="EM算法有两种常见的推导方式，一种是李航老师在《统计学习方法》里面给出的，另一种是吴恩达老师在CS229课上讲到的，两种推导方式各有千秋，但都殊途同归，下面对这两种推导方式进行一个总结。由于两种推导方式里都用到了一个经典的不等式——Jensen不等式[1]，所以下面先铺垫一下Jensen不等式。Jensen不等式的定义如下：若$f$是凸函数，则 f\left(t x_1 + (1-t)x_2\ri">
  
  
    <link rel="icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/typing.css">
  <link rel="stylesheet" href="/css/donate.css">
  
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/disqusjs@1.1/dist/disqusjs.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
</head>
</html>
  
    
      <body>
    
  
      <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container" class="container">
        <article id="post-expectation-maximization" class="article article-type-post" itemscope itemprop="blogPost">
  <header id="header" class="header">
  <nav class="mobile-nav">
    <h1 class="nickname">:)s</h1>
    <ul class="mobile-nav-menu">
      <label for="mobile-menu-toggle"><a>&#9776; Menu</a></label>
      <input type="checkbox" id="mobile-menu-toggle">
      <ul class="mobile-nav-link">
        
        <a href="/">Home</a>
        
        <a href="/archives">Archives</a>
        
        <a href="/about">About</a>
        
      </ul>
    </ul>
  </nav>
	
		<nav id="main-nav" class="main-nav nav-left">
	
	
	  <a class="main-nav-link" href="/">Home</a>
	
	  <a class="main-nav-link" href="/archives">Archives</a>
	
	  <a class="main-nav-link" href="/about">About</a>
	
  </nav>
</header>

  <hr>
  <div class="article-inner">
    

    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      EM算法的两种推导方式
    </h1>
  

      </header>
    
    <div class="article-entry typo" itemprop="articleBody">
      
        <p>EM算法有两种常见的推导方式，一种是李航老师在《统计学习方法》里面给出的，另一种是吴恩达老师在CS229课上讲到的，两种推导方式各有千秋，但都殊途同归，下面对这两种推导方式进行一个总结。由于两种推导方式里都用到了一个经典的不等式——Jensen不等式<sup><a href="#ref1">[1]</a></sup>，所以下面先铺垫一下Jensen不等式。Jensen不等式的定义如下：<br>若$f$是<strong>凸函数</strong>，则</p>
<script type="math/tex; mode=display">f\left(t x_1 + (1-t)x_2\right)\leq tf(x_1)+(1-t)f(x_2)</script><p>其中$t\in [0,1]$，若将$x$推广到$n$个时同样成立，即</p>
<script type="math/tex; mode=display">f(t_1 x_1 + t_2x_2+...+t_nx_n)\leq t_1f(x_1)+t_2f(x_2)+...+t_nf(t_n)</script><p>其中$t_1,t_2,…,t_n\in[0,1],t_1+t_2+…+t_n=1$。此不等式在概率论中通常以如下形式出现</p>
<script type="math/tex; mode=display">\varphi(E[X])\leq E[\varphi(X)]</script><p>其中$X$是一个随机变量，$\varphi$为凸函数，$E[X]$为随机变量$X$的期望。同理，若$f$和$\varphi$是<strong>凹函数</strong>，则只需将不等式中的$\leq$换成$\geq$即可。</p>
<h3 id="EM算法的由来"><a href="#EM算法的由来" class="headerlink" title="EM算法的由来"></a>EM算法的由来</h3><p>假设现有一批独立同分布的样本数据$\{x_1,x_2,…,x_m\}$，它们是由某个含有隐变量的模型$p(x,z;\theta)$生成，现尝试用极大似然估计法估计此模型的参数。由对数似然函数的定义可知此时的对数似然函数为（假设$z$为离散型）</p>
<script type="math/tex; mode=display">\begin{aligned} 
LL(\theta) &=\sum_{i=1}^{m} \ln p(x_i; \theta) \\ 
&=\sum_{i=1}^{m} \ln \sum_{z_i} p(x_i, z_i; \theta) 
\end{aligned}</script><p>显然，此时$LL(\theta)$里含有未知的隐变量$z$以及和（$z$为离散型时）或者积分（$z$为连续型时）的对数，因此无法按照传统方法直接求出使得$LL(\theta)$达到最大值的模型参数$\theta$，而EM算法则给出了一种迭代的方法来完成对$LL(\theta)$的极大化。下面给出EM算法的两种推导方式：</p>
<h3 id="《统计学习方法》版-2"><a href="#《统计学习方法》版-2" class="headerlink" title="《统计学习方法》版[2]"></a>《统计学习方法》版<sup><a href="#ref2">[2]</a></sup></h3><p>设$X=\{x_1,x_2,…,x_m\},Z=\{z_1,z_2,…,z_m\}$，则对数似然函数可以改写为</p>
<script type="math/tex; mode=display">\begin{aligned} 
LL(\theta)&=\ln P(X\vert \theta)\\
&=\ln \sum_Z P(X,Z\vert\theta)\\
&=\ln \left(\sum_Z P(X\vert Z,\theta)P(Z\vert \theta)\right)
\end{aligned}</script><p>EM算法采用的是通过迭代逐步近似极大化$L(\theta)$：假设第$t$次迭代时$\theta$的估计值是$\theta^{(t)}$，我们希望第$t+1$次迭代时的$\theta$能使$LL(\theta)$增大，也即$LL(\theta)&gt;LL(\theta^{(t)})$。为此，考虑两者的差</p>
<script type="math/tex; mode=display">\begin{aligned}
LL(\theta)-LL(\theta^{(t)})&=\ln \left(\sum_Z P(X\vert Z,\theta)P(Z\vert \theta)\right)-\ln P(X\vert\theta^{(t)}) \\
&=\ln \left(\sum_Z P(Z\vert X,\theta^{(t)}) \cfrac{P(X\vert Z,\theta)P(Z\vert \theta)}{P(Z\vert X,\theta^{(t)})}\right)-\ln P(X\vert\theta^{(t)})
\end{aligned}</script><p>由上述Jensen不等式可得</p>
<script type="math/tex; mode=display">\begin{aligned}
LL(\theta)-LL(\theta^{(t)})
&\geq \sum_Z P(Z\vert X,\theta^{(t)})\ln \cfrac{P(X\vert Z,\theta)P(Z\vert \theta)}{P(Z\vert X,\theta^{(t)})}-\ln P(X\vert\theta^{(t)}) \\
&= \sum_Z P(Z\vert X,\theta^{(t)})\ln \cfrac{P(X\vert Z,\theta)P(Z\vert \theta)}{P(Z\vert X,\theta^{(t)})}-1\cdot \ln P(X\vert\theta^{(t)}) \\
&= \sum_Z P(Z\vert X,\theta^{(t)})\ln \cfrac{P(X\vert Z,\theta)P(Z\vert \theta)}{P(Z\vert X,\theta^{(t)})}-\sum_Z P(Z\vert X,\theta^{(t)})\cdot \ln P(X\vert\theta^{(t)}) \\
&=\sum_Z P(Z\vert X,\theta^{(t)}) \left( \ln \cfrac{P(X\vert Z,\theta)P(Z\vert \theta)}{P(Z\vert X,\theta^{(t)})} - \ln P(X\vert\theta^{(t)}) \right)\\
&= \sum_Z P(Z\vert X,\theta^{(t)})\ln \cfrac{P(X\vert Z,\theta)P(Z\vert \theta)}{P(Z\vert X,\theta^{(t)})P(X\vert\theta^{(t)})}
\end{aligned}</script><p>令</p>
<script type="math/tex; mode=display">B(\theta,\theta^{(t)})=LL(\theta^{(t)})+\sum_Z P(Z\vert X,\theta^{(t)})\ln \cfrac{P(X\vert Z,\theta)P(Z\vert \theta)}{P(Z\vert X,\theta^{(t)})P(X\vert\theta^{(t)})}</script><p>则</p>
<script type="math/tex; mode=display">LL(\theta)\geq B(\theta,\theta^{(t)})</script><p>即$B(\theta,\theta^{(t)})$是$LL(\theta)$的下界，此时若设$\theta^{(t+1)}$能使得$B(\theta,\theta^{(t)})$达到极大，也即</p>
<script type="math/tex; mode=display">B(\theta^{(t+1)},\theta^{(t)}) \geq B(\theta,\theta^{(t)})</script><p>由于$LL(\theta^{(t)})=B(\theta^{(t)},\theta^{(t)})$，那么可以进一步推得</p>
<script type="math/tex; mode=display">LL(\theta^{(t+1)})\geq B(\theta^{(t+1)},\theta^{(t)})\geq B(\theta^{(t)},\theta^{(t)})=LL(\theta^{(t)})</script><script type="math/tex; mode=display">LL(\theta^{(t+1)})\geq LL(\theta^{(t)})</script><p>因此，任何能使得$B(\theta,\theta^{(t)})$增大的$\theta$，也可以使得$LL(\theta)$增大，于是问题就转化为了求解能使得$B(\theta,\theta^{(t)})$达到极大的$\theta^{(t+1)}$，即</p>
<script type="math/tex; mode=display">\begin{aligned}
\theta^{(t+1)}&=\mathop{\arg\max}_{\theta}B(\theta,\theta^{(t)}) \\
&=\mathop{\arg\max}_{\theta}\left( LL(\theta^{(t)})+\sum_Z P(Z\vert X,\theta^{(t)})\ln \cfrac{P(X\vert Z,\theta)P(Z\vert \theta)}{P(Z\vert X,\theta^{(t)})P(X\vert\theta^{(t)})}\right)
\end{aligned}</script><p>略去对$\theta$极大化而言是常数的项：</p>
<script type="math/tex; mode=display">\begin{aligned}
\theta^{(t+1)}&=\mathop{\arg\max}_{\theta}\left(\sum_Z P(Z\vert X,\theta^{(t)})\ln\left( P(X\vert Z,\theta)P(Z\vert \theta)\right)\right) \\
&=\mathop{\arg\max}_{\theta}\left(\sum_Z P(Z\vert X,\theta^{(t)})\ln P(X,Z\vert \theta)\right) \\
&=\mathop{\arg\max}_{\theta}Q(\theta,\theta^{(t)})
\end{aligned}</script><p>到此即完成了EM算法的一次迭代，求出的$\theta^{(t+1)}$作为下一次迭代的初始$\theta^{(t)}$。综上所述即可总结出EM算法的“E步”和“M步”分别为：</p>
<p><strong>E步：</strong>计算完全数据的对数似然函数$\ln P(X,Z\vert \theta)$关于在给定观测数据$X$和当前参数$\theta^{(t)}$下对未观测数据$Z$的条件概率分布$ P(Z\vert X,\theta^{(t)})$的期望，也即$Q(\theta,\theta^{(t)})$：</p>
<script type="math/tex; mode=display">Q(\theta,\theta^{(t)})=E_Z[\ln P(X,Z\vert \theta)\vert X,\theta^{(t)}]=\sum_Z P(Z\vert X,\theta^{(t)})\ln P(X,Z\vert \theta)</script><p><strong>M步：</strong>求使得$Q(\theta,\theta^{(t)})$达到极大的$\theta^{(t+1)}$。</p>
<h3 id="CS229版-3"><a href="#CS229版-3" class="headerlink" title="CS229版[3]"></a>CS229版<sup><a href="#ref3">[3]</a></sup></h3><p>设$z_i$的概率质量函数为$Q_i(z_i)$，则$LL(\theta)$可以作如下恒等变形</p>
<script type="math/tex; mode=display">\begin{aligned} 
LL(\theta) &=\sum_{i=1}^{m} \ln p(x_i; \theta) \\ 
&=\sum_{i=1}^{m} \ln \sum_{z_i} p(x_i, z_i; \theta) \\
&=\sum_{i=1}^{m} \ln \sum_{z_i} Q_i(z_i)\cfrac{p(x_i, z_i; \theta)}{Q_i(z_i)} \\
\end{aligned}</script><p>其中$\sum_{z_i} Q_i(z_i)\cfrac{p(x_i, z_i; \theta)}{Q_i(z_i)}$可以看做是对$\cfrac{p(x_i, z_i; \theta)}{Q_i(z_i)}$关于$z_i$求期望，也即</p>
<script type="math/tex; mode=display">\sum_{z_i} Q_i(z_i)\cfrac{p(x_i, z_i; \theta)}{Q_i(z_i)}=E_{z_i}\left[\cfrac{p(x_i, z_i; \theta)}{Q_i(z_i)}\right]</script><p>那么由Jensen不等式可得</p>
<script type="math/tex; mode=display">\ln\left(E_{z_i}\left[\cfrac{p(x_i, z_i; \theta)}{Q_i(z_i)}\right]\right)\geq E_{z_i}\left[\ln\left(\cfrac{p(x_i, z_i; \theta)}{Q_i(z_i)}\right)\right]</script><script type="math/tex; mode=display">\ln\sum_{z_i} Q_i(z_i)\cfrac{p(x_i, z_i; \theta)}{Q_i(z_i)}\geq \sum_{z_i} Q_i(z_i)\ln\cfrac{p(x_i, z_i; \theta)}{Q_i(z_i)}</script><p>将此式代入$LL(\theta)$可得</p>
<script type="math/tex; mode=display">\begin{aligned} 
LL(\theta) &=\sum_{i=1}^{m} \ln \sum_{z_i} Q_i(z_i)\cfrac{p(x_i, z_i; \theta)}{Q_i(z_i)} \qquad(A.1)\\
&\geq \sum_{i=1}^{m}\sum_{z_i} Q_i(z_i)\ln\cfrac{p(x_i, z_i; \theta)}{Q_i(z_i)}
\end{aligned}</script><p>若令$B(\theta)=\sum\limits_{i=1}^{m}\sum\limits_{z_i} Q_i(z_i)\ln\cfrac{p(x_i, z_i; \theta)}{Q_i(z_i)}$，则此时$B(\theta)$为$LL(\theta)$的下界函数，那么这个下界函数所能构成的最优下界是多少？也即$B(\theta)$的最大值是多少？显然，$B(\theta)$是$LL(\theta)$的下界函数，那么$LL(\theta)$就是其上界函数，所以如果能使得$B(\theta)=LL(\theta)$，那么此时的$B(\theta)$就取到了最大值。根据Jensen不等式的性质可知，如果能使得$\cfrac{p(x_i, z_i; \theta)}{Q_i(z_i)}$恒等于某个<strong>常量</strong>$c$，大于等于号便可以取到等号。因此，只需任意选取满足$\cfrac{p(x_i, z_i; \theta)}{Q_i(z_i)}=c$的$Q_i(z_i)$就能使得$B(\theta)$达到最大值。由于$Q_i(z_i)$是$z_i$的概率质量函数，所以$Q_i(z_i)$同时也满足约束$0\leq Q_i(z_i)\leq 1,\sum\limits_{z_i} Q_i(z_i)=1$，那么结合$Q_i(z_i)$的所有约束可以推得</p>
<script type="math/tex; mode=display">\cfrac{p(x_i, z_i; \theta)}{Q_i(z_i)}=c</script><script type="math/tex; mode=display">p(x_i, z_i; \theta)=c\cdot Q_i(z_i)</script><script type="math/tex; mode=display">\sum_{z_i}p(x_i, z_i; \theta)=c\cdot \sum_{z_i}Q_i(z_i)</script><script type="math/tex; mode=display">\sum_{z_i}p(x_i, z_i; \theta)=c</script><script type="math/tex; mode=display">\cfrac{p(x_i, z_i; \theta)}{Q_i(z_i)}=\sum_{z_i}p(x_i, z_i; \theta)</script><script type="math/tex; mode=display">Q_i(z_i)=\cfrac{p(x_i, z_i; \theta)}{\sum\limits_{z_i}p(x_i, z_i; \theta)}=\cfrac{p(x_i, z_i; \theta)}{p(x_i; \theta)}=p(z_i|x_i; \theta)</script><p>所以，当且仅当$Q_i(z_i)=p(z_i|x_i; \theta)$时$B(\theta)$取到最大值，将$Q_i(z_i)=p(z_i|x_i; \theta)$代回$LL(\theta)$和$B(\theta)$可以推得</p>
<script type="math/tex; mode=display">\begin{aligned} 
LL(\theta) &=\sum_{i=1}^{m} \ln \sum_{z_i} Q_i(z_i)\cfrac{p(x_i, z_i; \theta)}{Q_i(z_i)} &\qquad(A.2.1)\\
&=\sum_{i=1}^{m} \ln \sum_{z_i}p(z_i|x_i; \theta)\cfrac{p(x_i, z_i; \theta)}{p(z_i|x_i; \theta)} &\qquad(A.2.2)\\
&=\sum_{i=1}^{m}\sum_{z_i} p(z_i|x_i; \theta)\ln\cfrac{p(x_i, z_i; \theta)}{p(z_i|x_i; \theta)}&\qquad(A.2.3)\\
&=\max\{B(\theta)\} &\qquad(A.2.4)\\
\end{aligned}</script><p>其中，（A.2.3）是（A.1）中不等号取等号时的情形。由以上推导可知，对数似然函数$LL(\theta)$等价于其下界函数的最大值$\max\{B(\theta)\}$，所以要想极大化$LL(\theta)$可以通过极大化$\max\{B(\theta)\}$来间接极大化$LL(\theta)$，因此，下面考虑如何极大化$\max\{B(\theta)\}$。假设<strong>已知</strong>第$t$次迭代的参数为$\theta^{(t)}$，而第$t+1$次迭代的参数$\theta^{(t+1)}$通过如下方式求得</p>
<script type="math/tex; mode=display">\begin{aligned} 
\theta^{(t+1)}&=\arg\max_{\theta}\max\{B(\theta)\} \qquad\qquad\qquad\qquad\qquad(A.3) \\
&=\arg\max_{\theta}\sum_{i=1}^{m}\sum_{z_i} p(z_i|x_i;\theta^{(t)})\ln\cfrac{p(x_i, z_i; \theta)}{p(z_i|x_i; \theta^{(t)})} \\
&=\arg\max_{\theta}\sum_{i=1}^{m}\sum_{z_i} p(z_i|x_i;\theta^{(t)})\ln p(x_i, z_i; \theta)
\end{aligned}</script><p>将$\theta^{(t+1)}$代入$LL(\theta^{(t+1)})$则可以进一步推得</p>
<script type="math/tex; mode=display">\begin{aligned} 
LL(\theta^{(t+1)}) &=\max\{B(\theta^{(t+1)})\} &\qquad(A.4.1) \\
&=\sum_{i=1}^{m}\sum_{z_i} p(z_i|x_i; \theta^{(t+1)})\ln\cfrac{p(x_i, z_i; \theta^{(t+1)})}{p(z_i|x_i; \theta^{(t+1)})} &\qquad(A.4.2)\\
&\geq \sum_{i=1}^{m}\sum_{z_i} p(z_i|x_i; \theta^{(t)})\ln\cfrac{p(x_i, z_i; \theta^{(t+1)})}{p(z_i|x_i; \theta^{(t)})} &\qquad(A.4.3)\\
&\geq \sum_{i=1}^{m}\sum_{z_i} p(z_i|x_i; \theta^{(t)})\ln\cfrac{p(x_i, z_i; \theta^{(t)})}{p(z_i|x_i; \theta^{(t)})} &\qquad(A.4.4)\\
&=\max\{B(\theta^{(t)})\} &\qquad(A.4.5) \\
&=LL(\theta^{(t)})&\qquad(A.4.6)
\end{aligned}</script><p>其中，（A.4.1）和（A.4.2）由（A.2）推得；（A.4.3）由（A.1）推得；（A.4.4）由（A.3）推得；（A.4.5）和（A.4.6）由（A.2）推得。显然，若令</p>
<script type="math/tex; mode=display">Q(\theta,\theta^{(t)})=\sum_{i=1}^{m}\sum_{z_i} p(z_i|x_i; \theta^{(t)})\ln p(x_i, z_i; \theta)</script><p>那么由（A.4）可知，凡是能使得$Q(\theta,\theta^{(t)})$达到极大的$\theta^{(t+1)}$一定能使得$LL(\theta^{(t+1)})\geq LL(\theta^{(t)})$。综上所述即可总结出EM算法的“E步”和“M步”分别为：</p>
<p><strong>E步：</strong>令$Q_i(z_i)=p(z_i|x_i; \theta)$并写出$Q(\theta,\theta^{(t)})$；</p>
<p><strong>M步：</strong>求使得$Q(\theta,\theta^{(t)})$到达极大的$\theta^{(t+1)}$。</p>
<h3 id="证明两种推导方式中的Q函数相等"><a href="#证明两种推导方式中的Q函数相等" class="headerlink" title="证明两种推导方式中的Q函数相等"></a>证明两种推导方式中的Q函数相等</h3><script type="math/tex; mode=display">
\begin{aligned} Q(\theta|\theta^{(t)})&=\sum_Z P(Z|X,\theta^{(t)})\ln P(X,Z|\theta) \\
&=\sum_{z_1,z_2,...,z_m}\left\{\prod_{i=1}^mP(z_i|x_i,\theta^{(t)})\ln\left[ \prod_{i=1}^m P(x_i,z_i|\theta) \right] \right\} \\
&=\sum_{z_1,z_2,...,z_m}\left\{\prod_{i=1}^mP(z_i|x_i,\theta^{(t)})\left[ \sum_{i=1}^m\ln P(x_i,z_i|\theta) \right] \right\} \\
&=\sum_{z_1,z_2,...,z_m}\left\{\prod_{i=1}^mP(z_i|x_i,\theta^{(t)})\left[\ln P(x_1,z_1|\theta) + \ln P(x_2,z_2|\theta) +...+ \ln P(x_m,z_m|\theta)\right] \right\} \\
&=\sum_{z_1,z_2,...,z_m}\left[\prod_{i=1}^mP(z_i|x_i,\theta^{(t)})\cdot\ln P(x_1,z_1|\theta) \right]+...+\sum_{z_1,z_2,...,z_m}\left[\prod_{i=1}^mP(z_i|x_i,\theta^{(t)})\cdot\ln P(x_m,z_m|\theta) \right]  \\
\end{aligned}</script><p>其中$\sum\limits_{z_1,z_2,…,z_m}\left[\prod\limits_{i=1}^mP(z_i|x_i,\theta^{(t)})\cdot\ln P(x_1,z_1|\theta) \right] $可作如下恒等变形：</p>
<script type="math/tex; mode=display">\begin{aligned} 
&=\sum\limits_{z_1,z_2,...,z_m}\left[\prod_{i=2}^mP(z_i|x_i,\theta^{(t)})\cdot P(z_1|x_1,\theta^{(t)})\cdot\ln P(x_1,z_1|\theta) \right] \\
&=\sum_{z_1}\sum\limits_{z_2,...,z_m}\left[\prod_{i=2}^mP(z_i|x_i,\theta^{(t)})\cdot P(z_1|x_1,\theta^{(t)})\cdot\ln P(x_1,z_1|\theta) \right] \\
&=\sum_{z_1}P(z_1|x_1,\theta^{(t)})\ln P(x_1,z_1|\theta) \sum\limits_{z_2,...,z_m}\left[\prod_{i=2}^mP(z_i|x_i,\theta^{(t)}) \right] \\
&=\sum_{z_1}P(z_1|x_1,\theta^{(t)})\ln P(x_1,z_1|\theta)\sum\limits_{z_2,...,z_m}\left[\prod_{i=3}^mP(z_i|x_i,\theta^{(t)})\cdot P(z_2|x_2,\theta^{(t)}) \right] \\
&=\sum_{z_1}P(z_1|x_1,\theta^{(t)})\ln P(x_1,z_1|\theta) \left\{\sum\limits_{z_2}\sum\limits_{z_3,...,z_m}\left[\prod_{i=3}^mP(z_i|x_i,\theta^{(t)})\cdot P(z_2|x_2,\theta^{(t)}) \right]\right\} \\
&=\sum_{z_1}P(z_1|x_1,\theta^{(t)})\ln P(x_1,z_1|\theta) \left\{\sum_{z_2}P(z_2|x_2,\theta^{(t)}) \sum\limits_{z_3,...,z_m}\left[\prod_{i=3}^mP(z_i|x_i,\theta^{(t)})\right]\right\} \\
&=\sum_{z_1}P(z_1|x_1,\theta^{(t)})\ln P(x_1,z_1|\theta) \left\{\sum_{z_2}P(z_2|x_2,\theta^{(t)})\times\sum_{z_3}P(z_3|x_3,\theta^{(t)})\times...\times\sum_{z_m}P(z_m|x_m,\theta^{(t)})\right\} \\
&=\sum_{z_1}P(z_1|x_1,\theta^{(t)})\ln P(x_1,z_1|\theta)\times \left\{1\times1\times...\times1\right\} \\
&=\sum_{z_1}P(z_1|x_1,\theta^{(t)})\ln P(x_1,z_1|\theta)  \\
\end{aligned}</script><p>所以</p>
<script type="math/tex; mode=display">\sum\limits_{z_1,z_2,...,z_m}\left[\prod_{i=1}^mP(z_i|x_i,\theta^{(t)})\cdot\ln P(x_1,z_1|\theta) \right]=\sum_{z_1}P(z_1|x_1,\theta^{(t)})\ln P(x_1,z_1|\theta)</script><p>同理可得</p>
<script type="math/tex; mode=display">\begin{aligned} 
\sum\limits_{z_1,z_2,...,z_m}\left[\prod_{i=1}^mP(z_i|x_i,\theta^{(t)})\cdot\ln P(x_2,z_2|\theta) \right] &=\sum_{z_2}P(z_2|x_2,\theta^{(t)})\ln P(x_2,z_2|\theta) \\
&\vdots\\
\sum\limits_{z_1,z_2,...,z_m}\left[\prod_{i=1}^mP(z_i|x_i,\theta^{(t)})\cdot\ln P(x_m,z_m|\theta) \right] &=\sum_{z_m}P(z_m|x_m,\theta^{(t)})\ln P(x_m,z_m|\theta)
\end{aligned}</script><p>将上式代入$Q(\theta|\theta^{(t)})$可得</p>
<script type="math/tex; mode=display">\begin{aligned} 
Q(\theta|\theta^{(t)})&=\sum_{z_1,z_2,...,z_m}\left[\prod_{i=1}^mP(z_i|x_i,\theta^{(t)})\cdot\ln P(x_1,z_1|\theta) \right]+...+\sum_{z_1,z_2,...,z_m}\left[\prod_{i=1}^mP(z_i|x_i,\theta^{(t)})\cdot\ln P(x_m,z_m|\theta) \right]  \\
&=\sum_{z_1}P(z_1|x_1,\theta^{(t)})\ln P(x_1,z_1|\theta) +...+\sum_{z_m}P(z_m|x_m,\theta^{(t)})\ln P(x_m,z_m|\theta) \\
&=\sum_{i=1}^m\sum_{z_i}P(z_i|x_i,\theta^{(t)})\ln P(x_i,z_i|\theta)\\
\end{aligned}</script><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p><span id="ref1">[1] <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality" target="_blank" rel="noopener">Jensen’s inequality</a></span><br><span id="ref2">[2] 李航.《统计学习方法》第9章</span><br><span id="ref3">[3] Andrew Ng.CS229-notes8&lt;/a&gt;</span></p>

      
      
    </div>
    <footer class="article-footer">
      <ul class="article-meta">
        <li>
          <span class="label">本文作者：Sm1les</span>
        </li>
        <li>
          <span class="label">本文链接：<a href="http://sm1les.com/2019/03/13/expectation-maximization/">http://sm1les.com/2019/03/13/expectation-maximization/</a></span>
        </li>
        <li>
          <span class="label">版权声明：本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/3.0/">CC BY-NC-ND 3.0 许可协议</a>进行许可，转载请注明出处！</span>
        </li>
        <li>
          <span class="label">发布日期:</span>
          <a href="/2019/03/13/expectation-maximization/" class="article-date">
  <time datetime="2019-03-13T07:51:10.000Z" itemprop="datePublished">2019-03-13</time>
</a>

        </li>
        <li>
          <span class="label">更新日期:</span>
          <a href="/2019/03/13/expectation-maximization/" class="article-date">
  <time datetime="2019-10-16T05:25:56.919Z" itemprop="dateUpdated">2019-10-16</time>
</a>

        </li>
        
          <li>
            <span class="label">分类:</span>
            
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>


          </li>
        
        
          <li>
            <span class="label">标签:</span>
            
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/EM算法/">EM算法</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Jensen不等式/">Jensen不等式</a></li></ul>


          </li>
        
        <hr>
      </ul>
    </footer>
  </div>
  
    
<nav id="article-nav" class="article-nav">
  
    <a href="/2019/04/10/hidden-markov-model/" id="article-nav-newer" class="article-nav-link-wrap newer">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          隐马尔可夫模型（HMM）及其三个基本问题
        
      </div>
    </a>
  
  
    <a href="/2019/03/01/gradient-descent-and-newton-method/" id="article-nav-older" class="article-nav-link-wrap older">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">泰勒公式视角下的梯度下降法和牛顿法</div>
    </a>
  
</nav>


  
</article>


  <section id="comments" class="comments">
    <div id="disqus_thread"></div>
  </section>











      </div>
      
    <footer id="footer" class="post-footer footer">
      
      <hr>
      <div id="footerContent" class="footer-content">
        <p>© 2019 <strong>Sm1les</strong> Powered by <strong>Hexo</strong> Theme © <strong>GeekPlux</strong></p>


      </div>
    </footer>

      


<script src="https://cdn.jsdelivr.net/npm/disqusjs@1.1/dist/disqus.js"></script>
<script>
  var dsqjs = new DisqusJS({
      shortname: 'sm1les',
      siteName: 'sm1les',
      api: 'https://disqus.skk.moe/disqus/',
      apikey: 'NpJpGPceHYYAG6eEwrFFG6HE7SvnS5xaupPIG6CSTE67oyuOmQPTiFHMLtj3KkxO',
      admin: 'sm1lex,',
      adminLabel: 'Loading...'
  });
  </script>








<script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>


<script src="/js/typing.js"></script>
<!--[if lt IE 9]><script src="https://cdn.jsdelivr.net/npm/html5shiv@3/dist/html5shiv.min.js"></script><![endif]-->



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-131477813-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-131477813-1');
  </script>
  <!-- End Google Analytics -->
  




    </div>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
