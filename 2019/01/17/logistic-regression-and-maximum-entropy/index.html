<!DOCTYPE html>
<html lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Logistic回归与最大熵 | Sm1les&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="根据Wikipedia上的资料显示，Logistic回归的起源主要有以下几大历程：最早由Pierre François Verhulst在对人口增长情况进行研究时提出Logistic function[1]，后来Joseph Berkson在其基础上提出Logit function[2]，再后来David Cox用Logit function来做二分类的回归分析，进而提出了Logistic reg">
<meta name="keywords" content="最大熵,Logistic回归,广义线性模型">
<meta property="og:type" content="article">
<meta property="og:title" content="Logistic回归与最大熵">
<meta property="og:url" content="http://sm1les.com/2019/01/17/logistic-regression-and-maximum-entropy/index.html">
<meta property="og:site_name" content="Sm1les&#39;s blog">
<meta property="og:description" content="根据Wikipedia上的资料显示，Logistic回归的起源主要有以下几大历程：最早由Pierre François Verhulst在对人口增长情况进行研究时提出Logistic function[1]，后来Joseph Berkson在其基础上提出Logit function[2]，再后来David Cox用Logit function来做二分类的回归分析，进而提出了Logistic reg">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-07-23T13:44:54.738Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Logistic回归与最大熵">
<meta name="twitter:description" content="根据Wikipedia上的资料显示，Logistic回归的起源主要有以下几大历程：最早由Pierre François Verhulst在对人口增长情况进行研究时提出Logistic function[1]，后来Joseph Berkson在其基础上提出Logit function[2]，再后来David Cox用Logit function来做二分类的回归分析，进而提出了Logistic reg">
  
  
    <link rel="icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/typing.css">
  <link rel="stylesheet" href="/css/donate.css">
  
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/disqusjs@1.1/dist/disqusjs.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
</head>
</html>
  
    
      <body>
    
  
      <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container" class="container">
        <article id="post-logistic-regression-and-maximum-entropy" class="article article-type-post" itemscope itemprop="blogPost">
  <header id="header" class="header">
  <nav class="mobile-nav">
    <h1 class="nickname">:)s</h1>
    <ul class="mobile-nav-menu">
      <label for="mobile-menu-toggle"><a>&#9776; Menu</a></label>
      <input type="checkbox" id="mobile-menu-toggle">
      <ul class="mobile-nav-link">
        
        <a href="/">Home</a>
        
        <a href="/archives">Archives</a>
        
        <a href="/about">About</a>
        
      </ul>
    </ul>
  </nav>
	
		<nav id="main-nav" class="main-nav nav-left">
	
	
	  <a class="main-nav-link" href="/">Home</a>
	
	  <a class="main-nav-link" href="/archives">Archives</a>
	
	  <a class="main-nav-link" href="/about">About</a>
	
  </nav>
</header>

  <hr>
  <div class="article-inner">
    

    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Logistic回归与最大熵
    </h1>
  

      </header>
    
    <div class="article-entry typo" itemprop="articleBody">
      
        <p>根据Wikipedia上的资料显示，Logistic回归的起源主要有以下几大历程：最早由Pierre François Verhulst在对人口增长情况进行研究时提出Logistic function<sup><a href="#ref1">[1]</a></sup>，后来Joseph Berkson在其基础上提出Logit function<sup><a href="#ref2">[2]</a></sup>，再后来David Cox用Logit function来做二分类的回归分析，进而提出了Logistic regression<sup><a href="#ref3">[3]</a></sup>，详细的起源历程参见<a href="#ref4">[4]</a>和<a href="#ref5">[5]</a>。Logistic回归除了按照它的起源从对数几率的角度解释以外，还有业界比较认同的从最大熵的角度来解释，下面给出Logistic回归从最大熵角度的解释。</p>
<p>对于数据集$\{(\boldsymbol x_1,y_1),(\boldsymbol x_2,y_2)…(\boldsymbol x_m,y_m)\}$，其中$\boldsymbol x_i\in \mathbb{R}^n,i=1,2…m$，Logistic回归在对随机变量$y\vert\boldsymbol x$建模的时候是假设其取值仅为0或1，即$y_i\in\{0,1\}$，且$y\vert\boldsymbol x$有固定但未知的期望$\mu$，所以根据<strong>最大熵原理</strong>，此时可以假设$y\vert\boldsymbol x$服从伯努利分布（原因参见<a href="/2019/01/13/exponential-family-and-maximum-entropy">《指数族分布与最大熵》</a>），接着我们想用线性模型来对$y\vert\boldsymbol x$的概率$p(y\vert\boldsymbol x)$进行建模，于是可以通过广义线性模型（Generalized Linear Models）<sup><a href="#ref6">[6]</a></sup>的建模方法得到我们想要的模型。广义线性模型的建模步骤如下<sup><a href="#ref7">[7]</a></sup>：</p>
<ul>
<li>在给定$\boldsymbol x$的条件下，假设随机变量$y\vert\boldsymbol x$服从某个指数族分布（Exponential family）<sup><a href="#ref8">[8]</a></sup>；</li>
<li>假设该指数族分布中的自然参数$\eta(\boldsymbol\theta)$和$\boldsymbol x$呈线性关系，即$\eta(\boldsymbol\theta) = \boldsymbol w^T \boldsymbol x$；</li>
<li>建模出的模型为$T(y\vert\boldsymbol x)$的期望$E[T(y\vert\boldsymbol x)]$的表达式。</li>
</ul>
<p>在使用上述步骤对$y$进行建模前，先证明一下伯努利分布属于指数族分布：<br>伯努利分布的分布律如下：</p>
<script type="math/tex; mode=display">p(x)=\mu^x (1-\mu)^{1-x}</script><p>其中$x\in\{0,1\}$，$\mu$为$x=1$的概率也为$x$的期望，即$p(1)=E[x]=\mu$。对其进行恒等变形可得：</p>
<script type="math/tex; mode=display">
\begin{aligned}    
p(x) &= \mu^x (1-\mu)^{1-x} \\
&= \exp(x \ln \mu+(1−x) \ln(1−\mu)) \\
&= \exp \left((\ln(\cfrac{\mu}{1-\mu}))x+\ln(1-\mu) \right)
\end{aligned}</script><p>对照<a href="/2019/01/13/exponential-family-and-maximum-entropy">《指数族分布与最大熵》</a>中指数族分布的一般形式可知，伯努利分布属于指数族分布，且对应的指数族分布参数为：</p>
<script type="math/tex; mode=display">
\begin{aligned}    
\boldsymbol\theta &=\mu &(A.1)\\
\eta(\boldsymbol\theta)&= \ln(\cfrac{\mu}{1-\mu}) &(A.2)\\
T(x) &= x &(A.3)\\
A(\boldsymbol\theta) &= -\ln(1-\mu) &(A.4)\\
h(x) &= 1 &(A.5)
\end{aligned}</script><p>现在便可以根据上述广义线性模型的建模步骤对$y$进行建模：首先$y$服从伯努利分布，属于指数族分布，接着假设伯努利分布中的自然参数$\eta(\boldsymbol\theta)=\boldsymbol w^T \boldsymbol x$，再接着计算充分统计量$T(y\vert\boldsymbol x)$的期望表达式：</p>
<script type="math/tex; mode=display">
\begin{aligned}    
E[T(y\vert\boldsymbol x)]&= E[y\vert\boldsymbol x] \\
&= \mu \\
&= \cfrac{1}{1+e^{(-\eta(\boldsymbol\theta))}} \\
&= \cfrac{1}{1+e^{(-\boldsymbol w^T \boldsymbol x)}}
\end{aligned}</script><p>其中，第1个等式由式（A.3）导出；第2个等式是因为$y\vert\boldsymbol x$服从伯努利分布，所以$E(y\vert\boldsymbol x)=1*p(1\vert\boldsymbol x)+0*(1-p(1\vert\boldsymbol x))=p(1\vert\boldsymbol x)=\mu$；第3个等式是由式(A.2)导出，所以现在建模出的模型为：</p>
<script type="math/tex; mode=display">E(y\vert\boldsymbol x)=p(1\vert\boldsymbol x)=\cfrac{1}{1+e^{(-\boldsymbol w^T \boldsymbol x)}}</script><p>显然此即为Logistic回归模型。<br>【注】：</p>
<ul>
<li>上述广义线性模型的建模步骤是一种固定的建模方法，也就是说在构建广义线性模型时，我们唯一要做的就是假设$y\vert\boldsymbol x$服从何种<strong>指数族分布</strong>，通常是以最大熵原理为准则来假设$y\vert\boldsymbol x$的分布，例如在做二分类问题时通常假设$y\vert\boldsymbol x$服从伯努利分布，在做回归问题时通常假设$y\vert\boldsymbol x$服从高斯分布，在做网站访问量预测时通常假设$y\vert\boldsymbol x$服从泊松分布。在确定$y\vert\boldsymbol x$的分布后，只需按照上述步骤即可构建出一个广义线性模型；</li>
<li>除了从伯努利分布属于最大熵分布来解释以外，还有学者直接通过最大熵原理推导出Logistic回归，详细推导过程参见<a href="#ref9">[9]</a>，文中推导思路如下：首先说明Logistic回归是多分类模型类别总数k=2时的特例，所以只要用最大熵原理推导出多分类模型也就推导出了Logistic回归。于是先证明了多分类模型的Softmax函数在取得最优参数时类似一个指示函数，接着便以此为约束条件用最大熵原理推导出Softmax函数，也即推导出多分类模型，进而也就推导出了Logistic回归。类似的直接从最大熵原理出发的推导还有<a href="#ref10">[10]</a>；</li>
<li>Logistic回归也可以从贝叶斯的角度解释，参见<a href="#ref11">[11]</a></li>
<li>本文启发自知乎上的讨论：<a href="https://www.zhihu.com/question/35322351" target="_blank" rel="noopener">为什么 LR 模型要使用 sigmoid 函数，背后的数学原理是什么？</a></li>
</ul>
<h3 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h3><p><span id="ref1">[1] <a href="https://en.wikipedia.org/wiki/Logistic_function" target="_blank" rel="noopener">Logistic function</a></span><br><span id="ref2">[2] <a href="https://en.wikipedia.org/wiki/Logit" target="_blank" rel="noopener">Logit</a></span><br><span id="ref3">[3] <a href="https://en.wikipedia.org/wiki/Logistic_regression" target="_blank" rel="noopener">Logistic regression</a></span><br><span id="ref4">[4] <a href="https://chenrudan.github.io/blog/2016/01/09/logisticregression.html" target="_blank" rel="noopener">【机器学习算法系列之二】浅析Logistic Regression</a></span><br><span id="ref5">[5] Cramer J S . The Origins of Logistic Regression</span><br><span id="ref6">[6] <a href="https://en.wikipedia.org/wiki/Generalized_linear_model" target="_blank" rel="noopener">Generalized linear model</a></span><br><span id="ref7">[7] Andrew Ng. cs229-notes1</span><br><span id="ref8">[8] <a href="https://en.wikipedia.org/wiki/Exponential_family" target="_blank" rel="noopener">Exponential family</a></span><br><span id="ref9">[9] <a href="http://www. win-vector. com/dfiles/LogisticRegressionMaxEnt. pdf" target="_blank" rel="noopener">Mount, J.The equivalence of logistic regression and maximum entropy models</a></span><br><span id="ref10">[10] <a href="https://www.zhihu.com/question/24094554/answer/108271031" target="_blank" rel="noopener">如何理解最大熵模型里面的特征？ - Semiring的回答 - 知乎</a></span><br><span id="ref11">[11] <a href="http://charleshm.github.io/2016/03/LR-and-NB/" target="_blank" rel="noopener">Logistic Regression and Naive Bayes</a></span></p>

      
      
    </div>
    <footer class="article-footer">
      <ul class="article-meta">
        <li>
          <span class="label">本文作者：Sm1les</span>
        </li>
        <li>
          <span class="label">本文链接：<a href="http://sm1les.com/2019/01/17/logistic-regression-and-maximum-entropy/">http://sm1les.com/2019/01/17/logistic-regression-and-maximum-entropy/</a></span>
        </li>
        <li>
          <span class="label">版权声明：本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/3.0/">CC BY-NC-ND 3.0 许可协议</a>进行许可，转载请注明出处！</span>
        </li>
        <li>
          <span class="label">发布日期:</span>
          <a href="/2019/01/17/logistic-regression-and-maximum-entropy/" class="article-date">
  <time datetime="2019-01-17T14:22:03.000Z" itemprop="datePublished">2019-01-17</time>
</a>

        </li>
        <li>
          <span class="label">更新日期:</span>
          <a href="/2019/01/17/logistic-regression-and-maximum-entropy/" class="article-date">
  <time datetime="2019-07-23T13:44:54.738Z" itemprop="dateUpdated">2019-07-23</time>
</a>

        </li>
        
          <li>
            <span class="label">分类:</span>
            
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>


          </li>
        
        
          <li>
            <span class="label">标签:</span>
            
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Logistic回归/">Logistic回归</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/广义线性模型/">广义线性模型</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/最大熵/">最大熵</a></li></ul>


          </li>
        
        <hr>
      </ul>
    </footer>
  </div>
  
    
<nav id="article-nav" class="article-nav">
  
    <a href="/2019/03/01/gradient-descent-and-newton-method/" id="article-nav-newer" class="article-nav-link-wrap newer">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          梯度下降法与牛顿法
        
      </div>
    </a>
  
  
    <a href="/2019/01/13/exponential-family-and-maximum-entropy/" id="article-nav-older" class="article-nav-link-wrap older">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">指数族分布与最大熵</div>
    </a>
  
</nav>


  
</article>


  <section id="comments" class="comments">
    <div id="disqus_thread"></div>
  </section>











      </div>
      
    <footer id="footer" class="post-footer footer">
      
      <hr>
      <div id="footerContent" class="footer-content">
        <p>© 2019 <strong>Sm1les</strong> Powered by <strong>Hexo</strong> Theme © <strong>GeekPlux</strong></p>


      </div>
    </footer>

      


<script src="https://cdn.jsdelivr.net/npm/disqusjs@1.1/dist/disqus.js"></script>
<script>
  var dsqjs = new DisqusJS({
      shortname: 'sm1les',
      siteName: 'sm1les',
      api: 'https://disqus.skk.moe/disqus/',
      apikey: 'NpJpGPceHYYAG6eEwrFFG6HE7SvnS5xaupPIG6CSTE67oyuOmQPTiFHMLtj3KkxO',
      admin: 'sm1lex,',
      adminLabel: 'Loading...'
  });
  </script>








<script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>


<script src="/js/typing.js"></script>
<!--[if lt IE 9]><script src="https://cdn.jsdelivr.net/npm/html5shiv@3/dist/html5shiv.min.js"></script><![endif]-->



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-131477813-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-131477813-1');
  </script>
  <!-- End Google Analytics -->
  




    </div>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
