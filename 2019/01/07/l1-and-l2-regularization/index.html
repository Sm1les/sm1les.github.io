<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>L1正则化和L2正则化 | Sm1les&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项，正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。例如模型参数向量的范数，正则化的一般形式如下[1]： \min \limits_{f\in\mathcal{F}} \left( \cfrac{1}{N} \sum_{N}^{i=1} L(y_i,f(x_i))+\lambda J(f) \right) \qq">
<meta name="keywords" content="正则化">
<meta property="og:type" content="article">
<meta property="og:title" content="L1正则化和L2正则化">
<meta property="og:url" content="http://www.sm1les.com/2019/01/07/l1-and-l2-regularization/index.html">
<meta property="og:site_name" content="Sm1les&#39;s blog">
<meta property="og:description" content="正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项，正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。例如模型参数向量的范数，正则化的一般形式如下[1]： \min \limits_{f\in\mathcal{F}} \left( \cfrac{1}{N} \sum_{N}^{i=1} L(y_i,f(x_i))+\lambda J(f) \right) \qq">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://www.sm1les.com/2019/01/07/l1-and-l2-regularization/l1vsl2.png">
<meta property="og:image" content="http://www.sm1les.com/2019/01/07/l1-and-l2-regularization/normalandlaplace.png">
<meta property="og:updated_time" content="2019-03-10T10:04:29.768Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="L1正则化和L2正则化">
<meta name="twitter:description" content="正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项，正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。例如模型参数向量的范数，正则化的一般形式如下[1]： \min \limits_{f\in\mathcal{F}} \left( \cfrac{1}{N} \sum_{N}^{i=1} L(y_i,f(x_i))+\lambda J(f) \right) \qq">
<meta name="twitter:image" content="http://www.sm1les.com/2019/01/07/l1-and-l2-regularization/l1vsl2.png">
  
  
    <link rel="icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/typing.css">
  <link rel="stylesheet" href="/css/donate.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
</head>
</html>
  
    
      <body>
    
  
      <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container" class="container">
        <article id="post-l1-and-l2-regularization" class="article article-type-post" itemscope itemprop="blogPost">
  <header id="header" class="header">
  <nav class="mobile-nav">
    <h1 class="nickname">:)s</h1>
    <ul class="mobile-nav-menu">
      <label for="mobile-menu-toggle"><a>&#9776; Menu</a></label>
      <input type="checkbox" id="mobile-menu-toggle">
      <ul class="mobile-nav-link">
        
        <a href="/">Home</a>
        
        <a href="/archives">Archives</a>
        
        <a href="/about">About</a>
        
      </ul>
    </ul>
  </nav>
	
		<nav id="main-nav" class="main-nav nav-left">
	
	
	  <a class="main-nav-link" href="/">Home</a>
	
	  <a class="main-nav-link" href="/archives">Archives</a>
	
	  <a class="main-nav-link" href="/about">About</a>
	
  </nav>
</header>

  <hr>
  <div class="article-inner">
    

    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      L1正则化和L2正则化
    </h1>
  

      </header>
    
    <div class="article-entry typo" itemprop="articleBody">
      
        <p>正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项，正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。例如模型参数向量的范数，正则化的一般形式如下<sup><a href="#ref1">[1]</a></sup>：</p>
<script type="math/tex; mode=display">\min \limits_{f\in\mathcal{F}} \left( \cfrac{1}{N} \sum_{N}^{i=1} L(y_i,f(x_i))+\lambda J(f) \right) \qquad (A.1)</script><p>其中，第1项是经验风险，第2项是正则化项，$\lambda$为调整两者之间关系的系数。常用的正则化项是模型参数向量的$L_1$范数和$L_2$范数，分别称作L1正则化和L2正则化。以线性回归为例，其L1正则化的损失函数为：</p>
<script type="math/tex; mode=display">J(\boldsymbol w)=\cfrac{1}{N} \sum_{N}^{i=1} (f(\boldsymbol x_i;\boldsymbol w)-y_i)^2+\lambda \Vert\boldsymbol w \Vert_1</script><p>其中$\Vert\boldsymbol w \Vert_1$为$\boldsymbol w$的$L_1$范数，同理可得L2正则化的损失函数为：</p>
<script type="math/tex; mode=display">J(\boldsymbol w)=\cfrac{1}{N} \sum_{N}^{i=1} (f(\boldsymbol x_i;\boldsymbol w)-y_i)^2+\lambda \Vert\boldsymbol w \Vert_2^2</script><p>其中$\Vert\boldsymbol w \Vert_2$为$\boldsymbol w$的$L_2$范数，使用L1正则化或L2正则化的线性回归也称作<strong>LASSO回归</strong><sup><a href="#ref2">[2]</a></sup>或<strong>岭回归</strong><sup><a href="#ref3">[3]</a></sup>。<br>L1正则化和L2正则化最主要的不同之处在于前者更易得稀疏解，解释如下<sup><a href="#ref2">[2]</a></sup>：</p>
<h5 id="从优化问题的角度："><a href="#从优化问题的角度：" class="headerlink" title="从优化问题的角度："></a>从优化问题的角度：</h5><p>式(A.1)可以看作如下优化问题：</p>
<script type="math/tex; mode=display">\begin{aligned}
& \min\limits_{\boldsymbol w} \quad E_{emp}(\boldsymbol w) \\
& s.t.  \quad \lambda E_{reg}(\boldsymbol w) \leq \eta
\end{aligned}</script><p>其中$E_{emp}(\boldsymbol w)$是经验风险，$E_{reg}(\boldsymbol w)$是正则化项，$\eta$是自行设定的容忍度，此优化问题可以描述为：<strong>把$\boldsymbol w$的解限制一定范围内，同时使得经验风险尽可能小</strong>。L1正则化和L2正则化画图表示如下：</p>
<p><center>
<img src="./l1vsl2.png">
</center><br>其中，左图为L1正则化，右图为L2正则化，$\boldsymbol w^*$是$\boldsymbol w$的解，蓝色等高线为经验风险$E_{emp}(\boldsymbol w)$的取值，红色等高线为正则化项$E_{reg}(\boldsymbol w)$的取值，黄色区域是红色等高线的变化范围，也即$\boldsymbol w^*$的取值范围，默认内环等高线的值更小。从图中可以看出，红色等高线和蓝色等高线的<strong>切点</strong>即为优化问题的解$\boldsymbol w^*$，而且L1正则化相比于L2正则化更容易使得切点落在$\boldsymbol w$某个维度$w_i$的坐标轴上，从而导致另一个维度$w_j$的取值为0，从而更容易得到具有稀疏性的$\boldsymbol w^*$。</p>
<h5 id="从梯度的角度："><a href="#从梯度的角度：" class="headerlink" title="从梯度的角度："></a>从梯度的角度：</h5><p>L1正则化的损失函数一般形式为：</p>
<script type="math/tex; mode=display">J(\boldsymbol w)=L(\boldsymbol w)+\lambda \sum \vert w_i \vert \qquad (B.1)</script><p>L2正则化的损失函数一般形式为：</p>
<script type="math/tex; mode=display">J(\boldsymbol w)=L(\boldsymbol w)+\lambda \sum (w_i)^2 \qquad (B.2)</script><p>对式(B.1)关于$\boldsymbol w$某个维度$w_i$求偏导可得：</p>
<script type="math/tex; mode=display">\cfrac{\partial J(\boldsymbol w)}{\partial w_i}=\cfrac{\partial L(\boldsymbol w)}{\partial w_i}+\lambda sign (w_i)</script><p>对式(B.2)关于$\boldsymbol w$某个维度$w_i$求偏导可得：</p>
<script type="math/tex; mode=display">\cfrac{\partial J(\boldsymbol w)}{\partial w_i}=\cfrac{\partial L(\boldsymbol w)}{\partial w_i}+2\lambda w_i</script><p>当使用梯度下降法等此类根据$\boldsymbol w$的梯度来调整$\boldsymbol w$的算法时，若用L1正则化，$\boldsymbol w$的某个维度$w_i$的更新公式为：</p>
<script type="math/tex; mode=display">w_i:=w_i-\eta\cfrac{\partial J(\boldsymbol w)}{\partial w_i}=w_i-\eta\cfrac{\partial L(\boldsymbol w)}{\partial w_i}-\eta\lambda sign (w_i)</script><p>其中$\eta$为自行设定的学习率，从上式可以看出，$\eta\lambda sign (w_i)$的取值恒为$\pm\eta\lambda$，与$w_i$的大小无关，所以这就会导致即使$w_i$已经很小了但仍然以较高的梯度在变化，从而容易使得$w_i$取到0；若用L2正则化，则更新公式为：</p>
<script type="math/tex; mode=display">w_i:=w_i-\eta\cfrac{\partial J(\boldsymbol w)}{\partial w_i}=w_i-\eta\cfrac{\partial L(\boldsymbol w)}{\partial w_i}-2\eta\lambda w_i</script><p>显然此式中的$2\eta\lambda w_i$的大小与$w_i$相关，所以当$w_i$很小时变化的梯度也很小，不容易取到0，也就不容易得到稀疏解。</p>
<h5 id="从贝叶斯的角度："><a href="#从贝叶斯的角度：" class="headerlink" title="从贝叶斯的角度："></a>从贝叶斯的角度：</h5><p>贝叶斯学派常用的参数估计方法为最大后验估计（Maximum A Posteriori，MAP），最大后验估计法估计参数的公式为<sup><a href="#ref5">[5]</a></sup>：</p>
<script type="math/tex; mode=display">\begin{aligned}
\hat{\boldsymbol{w}} &= \mathop{\arg\max}_{\boldsymbol{w}}P(\boldsymbol{w} \vert \boldsymbol{y})\\
&= \mathop{\arg\max}_{\boldsymbol{w}}\log P(\boldsymbol{w} \vert \boldsymbol{y}) \\
&= \mathop{\arg\min}_{\boldsymbol{w}}-\log P(\boldsymbol{w} \vert \boldsymbol{y})
\end{aligned}</script><p>由贝叶斯公式可知$P(\boldsymbol{w} \vert \boldsymbol{y})=\cfrac{P(\boldsymbol{y} \vert \boldsymbol{w})\times P( \boldsymbol{w})}{P(\boldsymbol{y})}$，代入上式可得：</p>
<script type="math/tex; mode=display">\hat{\boldsymbol{w}} =\mathop{\arg\min}_{\boldsymbol{w}}-\log P(\boldsymbol{y} \vert \boldsymbol{w})-\log P(\boldsymbol{w})+\log P(\boldsymbol{y})</script><p>由于我们要求的是最优参数$\hat{\boldsymbol{w}}$，与$P(\boldsymbol{y})$无关，所以上式最后一项可以略去，进而得到损失函数为：</p>
<script type="math/tex; mode=display">J(\boldsymbol{w}) =-\log P(\boldsymbol{y} \vert \boldsymbol{w})-\log P(\boldsymbol{w})</script><p>其中$\log P(\boldsymbol{y} \vert \boldsymbol{w})$为对数似然函数，$P(\boldsymbol{w})$为我们对参数$\boldsymbol{w}$的先验估计，当我们先验估计参数$\boldsymbol{w}$服从均值为0方差为$\sigma^2$高斯分布时，即：</p>
<script type="math/tex; mode=display">\begin{aligned}
P(\boldsymbol{w}) &= \prod_{i=1}^{d}P(w_i)=\prod_{i=1}^{d}\cfrac{1}{\sqrt{2\pi}\sigma} \mathrm{exp}\left(-\cfrac{w_i^2}{2\sigma^2}\right) \\ 
\log P(\boldsymbol{w}) &= \sum_{i=1}^{d}\log\cfrac{1}{\sqrt{2\pi}\sigma} \mathrm{exp}\left(-\cfrac{w_i^2}{2\sigma^2}\right) \\
&= d \cdot \log\cfrac{1}{\sqrt{2\pi}\sigma} -\cfrac{1}{2\sigma^2} \cdot  \sum_{i=1}^{d}w_i^2 \\
&= d \cdot \log\cfrac{1}{\sqrt{2\pi}\sigma} -\cfrac{1}{2\sigma^2} \cdot  \Vert\boldsymbol{w}\Vert_2^2
\end{aligned}</script><p>代入损失函数$J(\boldsymbol{w})$可得：</p>
<script type="math/tex; mode=display">J(\boldsymbol{w}) = -\log P(\boldsymbol{y} \vert \boldsymbol{w})-d \cdot \log\cfrac{1}{\sqrt{2\pi}\sigma} +\cfrac{1}{2\sigma^2} \cdot  \Vert\boldsymbol{w}\Vert_2^2</script><p>同理$d \cdot \log\cfrac{1}{\sqrt{2\pi}\sigma}$与$\boldsymbol{w}$无关，可以略去，所以：</p>
<script type="math/tex; mode=display">J(\boldsymbol{w})=  -\log P(\boldsymbol{y} \vert \boldsymbol{w})+\cfrac{1}{2\sigma^2} \cdot  \Vert\boldsymbol{w}\Vert_2^2</script><p>此时可以看出<strong>采用高斯分布先验的最大后验估计=最大似然估计+L2正则项</strong>，同理可得<strong>采用拉普拉斯分布先验的最大后验估计=最大似然估计+L1正则项</strong>，观察高斯分布和拉普拉斯分布的概率密度函数图像可知，拉普拉斯分布更为稀疏，也即取到0的概率更大。</p>
<p><center>
<img src="./normalandlaplace.png" width="60%" height="60%"><br>
图中高斯分布和拉普拉斯分布的均值和方差均相同
</center><br>L1正则化和L2正则化还有如下不同之处：</p>
<ul>
<li>L1正则化自带特征选择的功能，这是由于L1正则化易得稀疏解导致的，因为稀疏解$\boldsymbol w^*$的某些维度$w_i=0$，从而达到了特征选择的功能；</li>
<li>L1正则化的解不稳定，也即可能会有多个解，这是因为L1正则化的红色等高线容易与经验风险的蓝色等高线产生多个切点，例如上图中的蓝色等高线若不为圆形曲线，而是直线时，此时极有可能与L1正则化的红色等高线重合，从而产生多个解；</li>
<li>L1正则化不易求解，这是因为绝对值函数通常都不好求解；</li>
<li>L1正则化相对于L2正则化对异常值敏感度低，这是因为当$\vert w_i \vert&gt;1$时，$\sum\vert w_i \vert &lt; \sum(w_i)^2$，从而对异常值敏感度低。</li>
</ul>
<h3 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h3><p><span id="ref1">[1] 李航.《统计学习方法》</span><br><span id="ref2">[2] <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)" target="_blank" rel="noopener">Lasso (statistics)</a></span><br><span id="ref3">[3] <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization" target="_blank" rel="noopener"> Ridge regression</a></span><br><span id="ref4">[4] <a href="https://www.zhihu.com/question/37096933" target="_blank" rel="noopener">l1 相比于 l2 为什么容易获得稀疏解？</a></span><br><span id="ref5">[5] <a href="https://zhuanlan.zhihu.com/p/32480810" target="_blank" rel="noopener">聊一聊机器学习的MLE和MAP：最大似然估计和最大后验估计</a></span></p>

      
      
    </div>
    <footer class="article-footer">
      <ul class="article-meta">
        <li>
          <span class="label">Published Date:</span>
          <a href="/2019/01/07/l1-and-l2-regularization/" class="article-date">
  <time datetime="2019-01-07T09:35:09.000Z" itemprop="datePublished">2019-01-07</time>
</a>

        </li>
        
          <li>
            <span class="label">Category:</span>
            
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>


          </li>
        
        
          <li>
            <span class="label">Tag:</span>
            
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/正则化/">正则化</a></li></ul>


          </li>
        
        <hr>
      </ul>
    </footer>
  </div>
  
    
<nav id="article-nav" class="article-nav">
  
    <a href="/2019/01/13/exponential-family-and-maximum-entropy/" id="article-nav-newer" class="article-nav-link-wrap newer">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          指数族分布与最大熵
        
      </div>
    </a>
  
  
    <a href="/2019/01/02/logistic-regression-log-loss-and-quadratic-loss/" id="article-nav-older" class="article-nav-link-wrap older">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Logistic回归——为什么用对数损失函数而不用平方损失函数？</div>
    </a>
  
</nav>


  
</article>










      </div>
      
    <footer id="footer" class="post-footer footer">
      
      <hr>
      <div id="footerContent" class="footer-content">
        <p>© 2019 <strong>Sm1les</strong> Powered by <strong>Hexo</strong> Theme © <strong>GeekPlux</strong></p>


      </div>
    </footer>

      







<script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>


<script src="/js/typing.js"></script>
<!--[if lt IE 9]><script src="https://cdn.jsdelivr.net/npm/html5shiv@3/dist/html5shiv.min.js"></script><![endif]-->



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-131477813-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-131477813-1');
  </script>
  <!-- End Google Analytics -->
  




    </div>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
