<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Logistic回归——为什么用对数损失函数而不用平方损失函数？ | Sm1les&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="损失函数本身作为选取最优模型的准则，当然是可以任意选取的，但是不同的模型选取场景下，各个准则的评价效果是不同的，因此需要因地制宜，选择评价效果最好的准则。 平方损失函数由最小二乘法可以推得Logistic回归的平方损失函数为[1]： J(\boldsymbol w)=\sum_{i=1}^{m} \left( y_i - \cfrac{1}{1+e^{-\boldsymbol w^T\boldsy">
<meta name="keywords" content="Logistic回归,损失函数">
<meta property="og:type" content="article">
<meta property="og:title" content="Logistic回归——为什么用对数损失函数而不用平方损失函数？">
<meta property="og:url" content="http://www.sm1les.com/2019/01/02/logistic-regression-log-loss-and-quadratic-loss/index.html">
<meta property="og:site_name" content="Sm1les&#39;s blog">
<meta property="og:description" content="损失函数本身作为选取最优模型的准则，当然是可以任意选取的，但是不同的模型选取场景下，各个准则的评价效果是不同的，因此需要因地制宜，选择评价效果最好的准则。 平方损失函数由最小二乘法可以推得Logistic回归的平方损失函数为[1]： J(\boldsymbol w)=\sum_{i=1}^{m} \left( y_i - \cfrac{1}{1+e^{-\boldsymbol w^T\boldsy">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-01-13T10:50:14.716Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Logistic回归——为什么用对数损失函数而不用平方损失函数？">
<meta name="twitter:description" content="损失函数本身作为选取最优模型的准则，当然是可以任意选取的，但是不同的模型选取场景下，各个准则的评价效果是不同的，因此需要因地制宜，选择评价效果最好的准则。 平方损失函数由最小二乘法可以推得Logistic回归的平方损失函数为[1]： J(\boldsymbol w)=\sum_{i=1}^{m} \left( y_i - \cfrac{1}{1+e^{-\boldsymbol w^T\boldsy">
  
  
    <link rel="icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/typing.css">
  <link rel="stylesheet" href="/css/donate.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
</head>
</html>
  
    
      <body>
    
  
      <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container" class="container">
        <article id="post-logistic-regression-log-loss-and-quadratic-loss" class="article article-type-post" itemscope itemprop="blogPost">
  <header id="header" class="header">
  <nav class="mobile-nav">
    <h1 class="nickname">:)s</h1>
    <ul class="mobile-nav-menu">
      <label for="mobile-menu-toggle"><a>&#9776; Menu</a></label>
      <input type="checkbox" id="mobile-menu-toggle">
      <ul class="mobile-nav-link">
        
        <a href="/">Home</a>
        
        <a href="/archives">Archives</a>
        
        <a href="/about">About</a>
        
      </ul>
    </ul>
  </nav>
	
		<nav id="main-nav" class="main-nav nav-left">
	
	
	  <a class="main-nav-link" href="/">Home</a>
	
	  <a class="main-nav-link" href="/archives">Archives</a>
	
	  <a class="main-nav-link" href="/about">About</a>
	
  </nav>
</header>

  <hr>
  <div class="article-inner">
    

    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Logistic回归——为什么用对数损失函数而不用平方损失函数？
    </h1>
  

      </header>
    
    <div class="article-entry typo" itemprop="articleBody">
      
        <p>损失函数本身作为选取最优模型的准则，当然是可以任意选取的，但是不同的模型选取场景下，各个准则的评价效果是不同的，因此需要因地制宜，选择评价效果最好的准则。</p>
<h3 id="平方损失函数"><a href="#平方损失函数" class="headerlink" title="平方损失函数"></a>平方损失函数</h3><p>由最小二乘法可以推得Logistic回归的平方损失函数为<sup><a href="#ref1">[1]</a></sup>：</p>
<script type="math/tex; mode=display">J(\boldsymbol w)=\sum_{i=1}^{m} \left( y_i - \cfrac{1}{1+e^{-\boldsymbol w^T\boldsymbol x_i}} \right)^2</script><p>此函数为<strong>非凸</strong>函数，易得局部最优解，不易求得全局最优解。</p>
<h3 id="对数损失函数"><a href="#对数损失函数" class="headerlink" title="对数损失函数"></a>对数损失函数</h3><p>对数损失函数也称为对数似然损失函数，一般由最（极）大似然估计法推得，推导过程如下：<br>设$y\in\{0,1\}$，$y$取1和0的概率分别分为$p_1(\boldsymbol{x};\boldsymbol w)$和$p_0(\boldsymbol{x};\boldsymbol w)$，则：</p>
<script type="math/tex; mode=display">\begin{aligned}    
p_1(\boldsymbol{x};\boldsymbol w) &= \cfrac{e^{\boldsymbol w^T\boldsymbol x}}{1+e^{\boldsymbol w^T\boldsymbol x}} \\
p_0(\boldsymbol{x};\boldsymbol w) &= 1-p_1(\boldsymbol{x};\boldsymbol w)=\cfrac{1}{1+e^{\boldsymbol w^T\boldsymbol x}}
\end{aligned}</script><p>似然项为：</p>
<script type="math/tex; mode=display">p(y|\boldsymbol x;\boldsymbol w)=[p_1(\boldsymbol{x};\boldsymbol w)]^{y}[p_0(\boldsymbol{x};\boldsymbol w)]^{1-y}</script><p>对数似然函数为：</p>
<script type="math/tex; mode=display">\begin{aligned}    
\ln L(\boldsymbol w) &= \sum_{i=1}^{m}\left(y_i\ln(p_1(\boldsymbol{x_i};\boldsymbol w))+(1-y_i)\ln(p_0(\boldsymbol{x_i};\boldsymbol w)) \right) \\
&= \sum_{i=1}^{m}\left(y_i \boldsymbol w^T\boldsymbol x_i-\ln(1+e^{\boldsymbol w^T\boldsymbol x_i}) \right) 
\end{aligned}</script><p>似然函数的目标是最大化函数值，而损失函数的目标是最小化函数值，所以将上式添加一个负号即可得对数损失函数：</p>
<script type="math/tex; mode=display">J(\boldsymbol w) = \sum_{i=1}^{m}\left(-y_i \boldsymbol w^T\boldsymbol x_i+\ln(1+e^{\boldsymbol w^T\boldsymbol x_i}) \right)</script><p>此函数为高阶连续可导凸函数，可以用各种凸优化算法求得全局最优解，这就是为什么用对数损失函数而不用平方损失函数的原因。<br>【注】：</p>
<ul>
<li>$y\in\{0,1\}$时两种似然项构造：<script type="math/tex; mode=display">\begin{aligned}
p(y|\boldsymbol x;\boldsymbol w) &= [p_1(\boldsymbol{x};\boldsymbol w)]^{y}[p_0(\boldsymbol{x};\boldsymbol w)]^{1-y} \\
p(y|\boldsymbol x;\boldsymbol w) &= yp_1(\boldsymbol{x};\boldsymbol w)+(1-y)p_0(\boldsymbol{x};\boldsymbol w)
\end{aligned}</script></li>
<li>$y\in\{1,-1\}$时一种似然项构造：<script type="math/tex; mode=display">p(y|\boldsymbol x;\boldsymbol w)=\cfrac{1}{1+e^{-y\boldsymbol w^T\boldsymbol x}}</script>其相应的对数似然函数为：<script type="math/tex; mode=display">\ln L(\boldsymbol w)=-\sum_{i=1}^{m}\ln(1+e^{-y_i\boldsymbol w^T\boldsymbol x_i})</script></li>
</ul>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p><span id="ref1">[1] <a href="https://www.zhihu.com/question/65350200" target="_blank" rel="noopener">逻辑回归损失函数为什么使用最大似然估计而不用最小二乘法？</a></span></p>

      
      
    </div>
    <footer class="article-footer">
      <ul class="article-meta">
        <li>
          <span class="label">Published Date:</span>
          <a href="/2019/01/02/logistic-regression-log-loss-and-quadratic-loss/" class="article-date">
  <time datetime="2019-01-02T10:14:17.000Z" itemprop="datePublished">2019-01-02</time>
</a>

        </li>
        
          <li>
            <span class="label">Category:</span>
            
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>


          </li>
        
        
          <li>
            <span class="label">Tag:</span>
            
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Logistic回归/">Logistic回归</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/损失函数/">损失函数</a></li></ul>


          </li>
        
        <hr>
      </ul>
    </footer>
  </div>
  
    
<nav id="article-nav" class="article-nav">
  
    <a href="/2019/01/07/l1-and-l2-regularization/" id="article-nav-newer" class="article-nav-link-wrap newer">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          L1正则化和L2正则化
        
      </div>
    </a>
  
  
    <a href="/2018/12/11/lagrange-duality/" id="article-nav-older" class="article-nav-link-wrap older">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">拉格朗日对偶性</div>
    </a>
  
</nav>


  
</article>










      </div>
      
    <footer id="footer" class="post-footer footer">
      
      <hr>
      <div id="footerContent" class="footer-content">
        <p>© 2019 <strong>Sm1les</strong> Powered by <strong>Hexo</strong> Theme © <strong>GeekPlux</strong></p>


      </div>
    </footer>

      







<script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>


<script src="/js/typing.js"></script>
<!--[if lt IE 9]><script src="https://cdn.jsdelivr.net/npm/html5shiv@3/dist/html5shiv.min.js"></script><![endif]-->



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-131477813-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-131477813-1');
  </script>
  <!-- End Google Analytics -->
  




    </div>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
