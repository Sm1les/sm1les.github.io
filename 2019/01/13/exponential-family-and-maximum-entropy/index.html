<!doctype html>
<html lang="en">
    <head><meta name="generator" content="Hexo 3.8.0">
		
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta http-equiv="Content-Type" content="text/html" charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="description" content>
        <link rel="shortcut icon" href="/styles/images/logo.jpg">
        <link rel="alternate" type="application/rss+xml" title="Sm1les" href="/atom.xml">

        <title>指数族分布与最大熵 | Sm1les&#39;s blog</title>
        <meta name="description" content="{{meta_description}}">

        <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="/styles/crisp.css">
        <meta name="HandheldFriendly" content="True">
        <meta name="MobileOptimized" content="320">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    </head>
    
		<body class="post-template">
	

        <!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="header">
            <a id="logo" href="/"><img src="/styles/images/logo.jpg" alt="Sm1les's blog"></a>
            <h1><a href="/">Sm1les</a></h1>
            <p></p>
            <div id="follow-icons">
	<a href="#"><i class="fa fa-instagram fa-2x"></i></a>
	<a href="#"><i class="fa fa-pinterest-square fa-2x"></i></a>
	<a href="http://github.com/Sm1les"><i class="fa fa-github-square fa-2x"></i></a>
	<a href="#"><i class="fa fa-facebook-square fa-2x"></i></a>
	<a href="#"><i class="fa fa-twitter-square fa-2x"></i></a>
	<a href="#"><i class="fa fa-linkedin-square fa-2x"></i></a>
	<a href="/atom.xml"><i class="fa fa-rss-square fa-2x"></i></a>
</div>
<div id="tag-cloud">

        <a href="/tags/EM算法/" style="font-size: 10px;">EM算法</a> <a href="/tags/KKT条件/" style="font-size: 10px;">KKT条件</a> <a href="/tags/Logistic回归/" style="font-size: 20px;">Logistic回归</a> <a href="/tags/Slater条件/" style="font-size: 10px;">Slater条件</a> <a href="/tags/对偶/" style="font-size: 10px;">对偶</a> <a href="/tags/广义线性模型/" style="font-size: 10px;">广义线性模型</a> <a href="/tags/指数族分布/" style="font-size: 10px;">指数族分布</a> <a href="/tags/损失函数/" style="font-size: 10px;">损失函数</a> <a href="/tags/最大熵/" style="font-size: 20px;">最大熵</a> <a href="/tags/梯度下降法/" style="font-size: 10px;">梯度下降法</a> <a href="/tags/正则化/" style="font-size: 10px;">正则化</a> <a href="/tags/牛顿法/" style="font-size: 10px;">牛顿法</a> <a href="/tags/线性回归/" style="font-size: 10px;">线性回归</a>
    
</div>
<h6><a href="/about">About</a></h6>

        </header>

        <main id="content">
        

<article class="post">
  
  一月 13, 2019
  
    <span class="taglist">  &middot; 
    
    
      <a href="/tags/指数族分布/">指数族分布</a> 
    
      <a href="/tags/最大熵/">最大熵</a> 
    
    </span>
  

  <h1 class="post-title">指数族分布与最大熵</h1>
  <section class="post-content article-entry">
    <h3 id="最大熵原理"><a href="#最大熵原理" class="headerlink" title="最大熵原理"></a>最大熵原理</h3><p>最大熵原理是概率模型学习的一个准则，最大熵原理认为：学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型<sup><a href="#ref1">[1]</a></sup>。此处所说的“熵”为信息论中的信息熵，信息熵定义如下：</p>
<ul>
<li>信息量：<script type="math/tex; mode=display">I(X) = -log_bp(X)</script>其中，$b=2$时单位为bit，$b=e$时单位为nat，$b=10$时单位为ban.</li>
<li>信息熵是信息量的期望，即：<script type="math/tex; mode=display">H(X)=E[I(X)]=E[-log_bp(X)]</script>当$X$为离散型时：<script type="math/tex; mode=display">H(X)=-\sum_x p(x)log_bp(x)</script>当$X$为连续型时：<script type="math/tex; mode=display">H(X)=-\int_{-\infty}^{+\infty}p(x)log_bp(x)dx</script>其中$p(x)=p(X=x)$，当$X$为连续型时信息熵也称为微分熵，熵只依赖于$X$的分布，而与$X$的取值无关。</li>
</ul>
<h3 id="指数族分布"><a href="#指数族分布" class="headerlink" title="指数族分布"></a>指数族分布</h3><p>指数族（Exponential family）分布<sup><a href="#ref2">[2]</a></sup>是一类分布的总称，该类分布的分布律（概率密度）的一般形式如下：</p>
<script type="math/tex; mode=display">\begin{aligned}    
p(x;\boldsymbol\theta) &= \cfrac{1}{Z(\boldsymbol\theta)} h(x) \exp\left(\eta(\boldsymbol\theta)^T T(x) \right) \\
&= h(x) \exp\left(\eta(\boldsymbol\theta)^T T(x) − A(\boldsymbol\theta)\right)
\end{aligned}</script><p>其中，$\boldsymbol\theta$为指数族分布的参数，视具体的分布而定，既可以是向量也可以是标量，此处暂用向量的形式表示，$\eta(\boldsymbol\theta)$是关于$\boldsymbol\theta$的函数，称作自然参数 (natural parameter，也称canonical parameter)，$T(x)$为充分统计量（sufficient statistic），$Z(\boldsymbol\theta)=\exp(-A(\boldsymbol\theta))$为配分函数（partition function），是用来保证分布律的累加和$\sum\limits_x p(x|\boldsymbol\theta)=1$或者概率密度的积分值$\int_{-\infty}^{+\infty}p(x|\boldsymbol\theta)=1$，$b(x)$为一个关于$x$的函数，常见的伯努利分布和正态分布均属于指数族分布，所有指数族分布以及每个分布的各项参数值参见<a href="#ref2">[2]</a>里的表格。指数族分布有个很重要的性质：在给定的约束条件下，指数族分布是信息熵（微分熵）最大的分布。例如：在已知$X\in\{0,1\}$且期望$E[X]=\mu$时，伯努利分布是熵最大的分布；在已知$X$的均值为$\mu$，方差为$\sigma^2$时，正态分布是熵最大的分布，其他最大熵分布参见<a href="#ref3">[3]</a>里的表格。</p>
<h5 id="指数族分布的最大熵推导："><a href="#指数族分布的最大熵推导：" class="headerlink" title="指数族分布的最大熵推导："></a>指数族分布的最大熵推导：</h5><p><strong>当$X$为离散型时</strong><sup><a href="#ref4">[4]</a></sup>：<br>若已知$X$满足如下约束条件：</p>
<script type="math/tex; mode=display">\sum_{k=1}^{n}\sum_{i=1}^{\vert X \vert} f_k(x_i)p(x_i) = F_k \qquad (A.1)</script><p>其中，$\vert X \vert$为$X$的可能取值个数，$n$为约束个数，$f_k(x_i)$为任意函数，$F_k$为已知常数，此时求$X$的最大熵分布等价于求解如下优化问题：</p>
<script type="math/tex; mode=display">\begin{aligned}    
\max\limits_{p}\quad&-\sum_{i=1}^{\vert X \vert} p(x_i)\ln p(x_i) \\
s.t.\quad &p(x_i)  \geq0 \\
&\sum_{i=1}^{\vert X \vert} p(x_i) = 1 \\
&\sum_{k=1}^{n}\sum_{i=1}^{\vert X \vert}  f_k(x_i)p(x_i) = F_k
\end{aligned}</script><p>其中信息熵的单位为nat，也即取$b=e$，对该优化问题用拉格朗日乘子法可得：</p>
<script type="math/tex; mode=display">\begin{aligned}    
L(p,\boldsymbol\lambda) &=-\sum_{i=1}^{\vert X \vert}p(x_i)\ln p(x_i)+\lambda_0(1-\sum_{i=1}^{\vert X \vert}p(x_i))+\sum_{k=1}^{n}\lambda_k\left(F_k-\sum_{i=1}^{\vert X \vert} f_k(x_i)p(x_i) \right) \\
&=\sum_{i=1}^{\vert X \vert} -p(x_i)\ln p(x_i)-\sum_{i=1}^{\vert X \vert} \lambda_0 p(x_i)-\sum_{i=1}^{\vert X \vert}\sum_{k=1}^{n}\lambda_kf_k(x_i)p(x_i)+\lambda_0+\sum_{k=1}^{n}\lambda_kF_k \\
&=\sum_{i=1}^{\vert X \vert}\left(-p(x_i)\ln p(x_i)-\lambda_0 p(x_i)-\sum_{k=1}^{n}\lambda_kf_k(x_i)p(x_i)\right)+\lambda_0+\sum_{k=1}^{n}\lambda_kF_k
\end{aligned}</script><p>其中，$p$可以看作一个分布律向量，也即$p=[p(x_1),p(x_2),…,p(x_{\vert X \vert})]$，$\boldsymbol\lambda=[\lambda_0,\lambda_1,…,\lambda_n]^T$为拉格朗日乘子向量，对$L(p,\boldsymbol\lambda)$关于$p$求偏导等价于分别对所有的$p(x_i)$求偏导：</p>
<script type="math/tex; mode=display">\begin{aligned}
\cfrac{\partial L(p,\boldsymbol\lambda)}{\partial p(x_1)}&=-\ln p(x_1)-1-\lambda_0-\sum_{k=1}^{n}\lambda_kf_k(x_1) \\
\cfrac{\partial L(p,\boldsymbol\lambda)}{\partial p(x_2)}&=-\ln p(x_2)-1-\lambda_0-\sum_{k=1}^{n}\lambda_kf_k(x_2) \\
\vdots \\
\cfrac{\partial L(p,\boldsymbol\lambda)}{\partial p(x_{\vert X \vert})}&=-\ln p(x_{\vert X \vert})-1-\lambda_0-\sum_{k=1}^{n}\lambda_kf_k(x_{\vert X \vert}) 
\end{aligned}</script><p>则</p>
<script type="math/tex; mode=display">\cfrac{\partial L(p,\boldsymbol\lambda)}{\partial p(x_i)}=-\ln p(x_i)-1-\lambda_0-\sum_{k=1}^{n}\lambda_kf_k(x_i)</script><p>令上式等于0解得：</p>
<script type="math/tex; mode=display">p(x_i) =\exp(-1-\lambda_0-\sum\limits_{k=1}^{n}\lambda_kf_k(x_i))</script><p>又由约束条件$\sum\limits_{i=1}^{\vert X \vert} p(x_i) = 1$可得：</p>
<script type="math/tex; mode=display">\begin{aligned}    
\sum_{i=1}^{\vert X \vert} \exp(-1-\lambda_0-\sum\limits_{k=1}^{n}\lambda_kf_k(x_i))= 1 \\
\sum_{i=1}^{\vert X \vert} \cfrac{\exp(-\sum\limits_{k=1}^{n}\lambda_kf_k(x_i))}{e^{(1+\lambda_0)}}= 1 \\
e^{(1+\lambda_0)}=\sum_{i=1}^{\vert X \vert} \exp(-\sum_{k=1}^{n}\lambda_kf_k(x_i))
\end{aligned}</script><p>将其代入$p(x_i)$可得：</p>
<script type="math/tex; mode=display">p(x_i) =\cfrac{\exp(-\sum\limits_{k=1}^{n}\lambda_kf_k(x_i))}{\sum\limits_{i=1}^{\vert X \vert} \exp(-\sum\limits_{k=1}^{n}\lambda_kf_k(x_i))}</script><p>此式为$X$取到各个值$x_i$的概率，可以从中抽象出$X$的分布律为：</p>
<script type="math/tex; mode=display">\begin{aligned}    
p(x) &=\cfrac{\exp(-\sum\limits_{k=1}^{n}\lambda_kf_k(x))}{\sum\limits_x\exp(-\sum\limits_{k=1}^{n}\lambda_kf_k(x))} \qquad (A.2) \\
&=\cfrac{1}{Z}\exp(-\sum\limits_{k=1}^{n}\lambda_kf_k(x))
\end{aligned}</script><p>其中$Z=\sum\limits_x\exp(-\sum\limits_{k=1}^{n}\lambda_kf_k(x))$，此时$p(x)$的表达式显然符合指数族分布的一般形式。（注：上述推导过程是结合<a href="#ref1">[1]</a>中例6.2和<a href="#ref4">[4]</a>中9.2.6所述内容而成）<br><strong>当$X$为连续型时</strong>：<br>若已知$X$满足如下约束条件：</p>
<script type="math/tex; mode=display">\sum_{k=1}^{n}\int_{-\infty}^{+\infty}f_k(x)p(x)dx = F_k \qquad (B.1)</script><p>其中，$n$为约束个数，$f_k(x)$为任意函数，$F_k$为已知常数，此时求$X$的最大熵分布等价于求解如下优化问题：</p>
<script type="math/tex; mode=display">\begin{aligned}    
\max\limits_{p}\quad&-\int_{-\infty}^{+\infty}p(x)\ln p(x)dx \\
s.t.\quad &p(x)  \geq0 \\
&\int_{-\infty}^{+\infty}p(x)dx = 1 \\
&\int_{-\infty}^{+\infty}f_k(x)p(x)dx = F_k
\end{aligned}</script><p>其中信息熵的单位为nat，也即取$b=e$，对该优化问题用拉格朗日乘子法可得：</p>
<script type="math/tex; mode=display">\begin{aligned}    
L(p,\boldsymbol\lambda) &=-\int_{-\infty}^{+\infty}p(x)\ln p(x)dx+\lambda_0(1-\int_{-\infty}^{+\infty}p(x)dx)+\sum_{k=1}^{n}\lambda_k\left(F_k-\int_{-\infty}^{+\infty}f_k(x)p(x)dx \right) \\
&=\int_{-\infty}^{+\infty}-p(x)\ln p(x)dx-\int_{-\infty}^{+\infty}\lambda_0 p(x)dx-\int_{-\infty}^{+\infty}\sum_{k=1}^{n}\lambda_kf_k(x)p(x)dx+\lambda_0+\sum_{k=1}^{n}\lambda_kF_k \\
&=\int_{-\infty}^{+\infty}\left(-p(x)\ln p(x)-\lambda_0 p(x)-\sum_{k=1}^{n}\lambda_kf_k(x)p(x)\right)dx+\lambda_0+\sum_{k=1}^{n}\lambda_kF_k
\end{aligned}</script><p>其中，$\int_{-\infty}^{+\infty}$可以看作$\sum\limits_x$，因此可以按照$X$为离散型时的推导方法推得$X$的分布律为：</p>
<script type="math/tex; mode=display">\begin{aligned}    
p(x) &=\cfrac{\exp(-\sum\limits_{k=1}^{n}\lambda_kf_k(x))}{\int_{-\infty}^{+\infty}\exp(-\sum\limits_{k=1}^{n}\lambda_kf_k(x))dx} \qquad (B.2)\\
&=\cfrac{1}{Z}\exp(-\sum\limits_{k=1}^{n}\lambda_kf_k(x))
\end{aligned}</script><p>其中$Z=\int_{-\infty}^{+\infty}\exp(-\sum\limits_{k=1}^{n}\lambda_kf_k(x))dx$。</p>
<h5 id="伯努利分布的最大熵推导："><a href="#伯努利分布的最大熵推导：" class="headerlink" title="伯努利分布的最大熵推导："></a>伯努利分布的最大熵推导：</h5><p><strong>“已知$X\in\{0,1\}$且期望$E[X]=\mu$时，伯努利分布是熵最大的分布”</strong>，证明如下：<br>已知$X\in\{0,1\}$，所以$X$属于离散型，则根据式（A.2）知$X$的分布律的一般形式为：</p>
<script type="math/tex; mode=display">p(x) =\cfrac{\exp(-\sum\limits_{k=1}^{n}\lambda_kf_k(x))}{\exp(-\sum\limits_{k=1}^{n}\lambda_kf_k(0))+\exp(-\sum\limits_{k=1}^{n}\lambda_kf_k(1))}</script><p>又已知$E[X]=\mu$，其等价于如下约束条件：</p>
<script type="math/tex; mode=display">\sum_{i=1}^{\vert X \vert} x_ip(x_i) =\mu</script><p>所以对比式（A.2）可知，此时$n=1,f_1(x_i)=x_i,F_1=\mu$，代入$p(x)$可得：</p>
<script type="math/tex; mode=display">p(x) =\cfrac{e^{-\lambda_1 x}}{1+e^{-\lambda_1}}</script><p>再由$E[X]=\mu$可得：</p>
<script type="math/tex; mode=display">E[X]=0*p(0)+1*p(1)=p(1)=\cfrac{e^{-\lambda_1}}{1+e^{-\lambda_1}}=\mu</script><p>所以$X$的分布律为：</p>
<script type="math/tex; mode=display">\begin{aligned}    
p(0)&=1-\mu \\
p(1)&=\mu
\end{aligned}</script><p>显然此即为伯努利分布，证毕。</p>
<h5 id="正态分布的最大熵推导："><a href="#正态分布的最大熵推导：" class="headerlink" title="正态分布的最大熵推导："></a>正态分布的最大熵推导：</h5><p><strong>“已知$X$的均值为$\mu$，方差为$\sigma^2$时，正态分布是熵最大的分布”</strong>，证明如下<sup><a href="#ref5">[5]</a></sup>：<br>此时没有限定$X$的取值范围，则默认为$X\in(-\infty,+\infty)$的连续型，已知$X$的均值为$\mu$，方差为$\sigma^2$，其等价于如下约束条件：</p>
<script type="math/tex; mode=display">\int_{-\infty}^{+\infty}(x-\mu)^2p(x)dx = \sigma^2</script><p>所以对比式（B.1）可知，此时$n=1,f_1(x)=(x-\mu)^2,F_1=\sigma^2$，代入式（B.1）可得此时$X$的分布律：</p>
<script type="math/tex; mode=display">p(x) =\cfrac{\exp(-\lambda_1 (x-\mu)^2)}{\int_{-\infty}^{+\infty}\exp(-\lambda_1 (x-\mu)^2)dx}</script><p>再由$\int_{-\infty}^{+\infty}(x-\mu)^2p(x)dx = \sigma^2$解得：</p>
<script type="math/tex; mode=display">\lambda_1=\cfrac{1}{2\sigma^2}</script><p>代入$p(x)$中得：</p>
<script type="math/tex; mode=display">p(x) =\cfrac{1}{\sqrt{2\pi}\sigma}\exp\left(-\cfrac{(x-\mu)^2}{2\sigma^2}\right)</script><p>显然此即为正态分布，证毕。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p><span id="ref1">[1] 李航.《统计学习方法》</span><br><span id="ref2">[2] <a href="https://en.wikipedia.org/wiki/Exponential_family" target="_blank" rel="noopener">Exponential family</a></span><br><span id="ref3">[3] <a href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution" target="_blank" rel="noopener">Maximum entropy probability distribution</a></span><br><span id="ref4">[4] Murphy K P. 《Machine Learning: A Probabilistic Perspective》</span><br><span id="ref5">[5] 周晓飞. 《统计机器学习》课程的课件</span></p>

  </section>
  <footer class="post-footer">
    <!--
    <section class="author">
      <h4>Sm1les</h4>
      <p></p>
    </section>
    -->
  </footer>
</article>

<nav class="pagination" role="pagination">
    
    <a class="newer-posts" href="/2019/01/17/logistic-regression-and-maximum-entropy/">
        ← prev <!--Logistic回归与最大熵-->
    </a>
    
    <span class="page-number">•</span>
    
    <a class="older-posts" href="/2019/01/07/l1-and-l2-regularization/">
        <!--L1正则化和L2正则化--> next →
    </a>
    
</nav>

<!-- Begin Comments Code -->

  <div id="comments">
    <!-- 多说评论框 start -->
    <div class="ds-thread" data-thread-key="post-exponential-family-and-maximum-entropy" data-title="指数族分布与最大熵" data-url="http://www.sm1les.com/2019/01/13/exponential-family-and-maximum-entropy/"></div>
    <!-- 多说评论框 end -->
    <!-- 多说公共JS代码 start (一个网页只需插入一次) -->
    <script type="text/javascript">
    var duoshuoQuery = {short_name:'sm1les'};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] 
         || document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
      </script>
    <!-- 多说公共JS代码 end -->
  </div>

<!-- End Comments Code -->


        </main>
        <footer id="footer">
            <section id="footer-message">&copy; 2019 Sm1les. All rights reserved. Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. <a href="https://github.com/guolin/crisp-hexo-theme" target="_blank">crisp</a> theme by <a href="http://guolin.github.io" target="_blank">Guo Lin</a>.</section>
        </footer>
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','/js/ga.js','ga');
            ga('create', 'UA-131477813-1', 'auto');
            ga('send', 'pageview');
        </script>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>


