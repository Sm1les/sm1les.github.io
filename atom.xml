<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Sm1les&#39;s blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.sm1les.com/"/>
  <updated>2019-01-09T08:00:52.680Z</updated>
  <id>http://www.sm1les.com/</id>
  
  <author>
    <name>Sm1les</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>L1正则化和L2正则化</title>
    <link href="http://www.sm1les.com/2019/01/07/l1-and-l2-regularization/"/>
    <id>http://www.sm1les.com/2019/01/07/l1-and-l2-regularization/</id>
    <published>2019-01-07T09:35:09.000Z</published>
    <updated>2019-01-09T08:00:52.680Z</updated>
    
    <content type="html"><![CDATA[<p>正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项，正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。例如模型参数向量的范数，正则化的一般形式如下<sup><a href="#ref1">[1]</a></sup>：</p>
<script type="math/tex; mode=display">\min \limits_{f\in\mathcal{F}} \left( \cfrac{1}{N} \sum_{N}^{i=1} L(y_i,f(x_i))+\lambda J(f) \right) \qquad (A.1)</script><p>其中，第1项是经验风险，第2项是正则化项，$\lambda$为调整两者之间关系的系数。常用的正则化项是模型参数向量的$L_1$范数和$L_2$范数，分别称作L1正则化和L2正则化。以线性回归为例，其L1正则化的损失函数为：</p>
<script type="math/tex; mode=display">J(\boldsymbol w)=\cfrac{1}{N} \sum_{N}^{i=1} (f(\boldsymbol x_i;\boldsymbol w)-y_i)^2+\lambda \Vert\boldsymbol w \Vert_1</script><p>其中$\Vert\boldsymbol w \Vert_1$为$\boldsymbol w$的$L_1$范数，同理可得L2正则化的损失函数为：</p>
<script type="math/tex; mode=display">J(\boldsymbol w)=\cfrac{1}{N} \sum_{N}^{i=1} (f(\boldsymbol x_i;\boldsymbol w)-y_i)^2+\lambda \Vert\boldsymbol w \Vert_2^2</script><p>其中$\Vert\boldsymbol w \Vert_2$为$\boldsymbol w$的$L_2$范数，使用L1正则化或L2正则化的线性回归也称作<strong>LASSO回归</strong><sup><a href="#ref2">[2]</a></sup>或<strong>岭回归</strong><sup><a href="#ref3">[3]</a></sup>。<br>L1正则化和L2正则化最主要的不同之处在于前者更易得稀疏解，解释如下<sup><a href="#ref2">[2]</a></sup>：</p>
<h5 id="从优化问题的角度："><a href="#从优化问题的角度：" class="headerlink" title="从优化问题的角度："></a>从优化问题的角度：</h5><p>式(A.1)可以看作如下优化问题：</p>
<script type="math/tex; mode=display">\begin{aligned}
& \min\limits_{\boldsymbol w} \quad E_{emp}(\boldsymbol w) \\
& s.t.  \quad \lambda E_{reg}(\boldsymbol w) \leq \eta
\end{aligned}</script><p>其中$E_{emp}(\boldsymbol w)$是经验风险，$E_{reg}(\boldsymbol w)$是正则化项，$\eta$是自行设定的容忍度，此优化问题可以描述为：<strong>把$\boldsymbol w$的解限制一定范围内，同时使得经验风险尽可能小</strong>。L1正则化和L2正则化画图表示如下：</p>
<p><center>
<img src="./l1vsl2.png">
</center><br>其中，左图为L1正则化，右图为L2正则化，$\boldsymbol w^*$是$\boldsymbol w$的解，蓝色等高线为经验风险$E_{emp}(\boldsymbol w)$的取值，红色等高线为正则化项$E_{reg}(\boldsymbol w)$的取值，黄色区域是红色等高线的变化范围，也即$\boldsymbol w^*$的取值范围，默认内环等高线的值更小。从图中可以看出，红色等高线和蓝色等高线的<strong>切点</strong>即为优化问题的解$\boldsymbol w^*$，而且L1正则化相比于L2正则化更容易使得切点落在$\boldsymbol w$某个维度$w_i$的坐标轴上，从而导致另一个维度$w_j$的取值为0，从而更容易得到具有稀疏性的$\boldsymbol w^*$。</p>
<h5 id="从梯度的角度："><a href="#从梯度的角度：" class="headerlink" title="从梯度的角度："></a>从梯度的角度：</h5><p>L1正则化的损失函数一般形式为：</p>
<script type="math/tex; mode=display">J(\boldsymbol w)=L(\boldsymbol w)+\lambda \sum \vert w_i \vert \qquad (B.1)</script><p>L2正则化的损失函数一般形式为：</p>
<script type="math/tex; mode=display">J(\boldsymbol w)=L(\boldsymbol w)+\lambda \sum (w_i)^2 \qquad (B.2)</script><p>对式(B.1)关于$\boldsymbol w$某个维度$w_i$求偏导可得：</p>
<script type="math/tex; mode=display">\cfrac{\partial J(\boldsymbol w)}{\partial w_i}=\cfrac{\partial L(\boldsymbol w)}{\partial w_i}+\lambda sign (w_i)</script><p>对式(B.2)关于$\boldsymbol w$某个维度$w_i$求偏导可得：</p>
<script type="math/tex; mode=display">\cfrac{\partial J(\boldsymbol w)}{\partial w_i}=\cfrac{\partial L(\boldsymbol w)}{\partial w_i}+2\lambda w_i</script><p>当使用梯度下降法等此类根据$\boldsymbol w$的梯度来调整$\boldsymbol w$的算法时，若用L1正则化，$\boldsymbol w$的某个维度$w_i$的更新公式为：</p>
<script type="math/tex; mode=display">w_i:=w_i-\eta\cfrac{\partial J(\boldsymbol w)}{\partial w_i}=w_i-\eta\cfrac{\partial L(\boldsymbol w)}{\partial w_i}-\eta\lambda sign (w_i)</script><p>其中$\eta$为自行设定的学习率，从上式可以看出，$\eta\lambda sign (w_i)$的取值恒为$\pm\eta\lambda$，与$w_i$的大小无关，所以这就会导致即使$w_i$已经很小了但仍然以较高的梯度在变化，从而容易使得$w_i$取到0；若用L2正则化，则更新公式为：</p>
<script type="math/tex; mode=display">w_i:=w_i-\eta\cfrac{\partial J(\boldsymbol w)}{\partial w_i}=w_i-\eta\cfrac{\partial L(\boldsymbol w)}{\partial w_i}-2\eta\lambda w_i</script><p>显然此式中的$2\eta\lambda w_i$的大小与$w_i$相关，所以当$w_i$很小时变化的梯度也很小，不容易取到0，也就不容易得到稀疏解。<br>L1正则化和L2正则化还有如下不同之处：</p>
<ul>
<li>L1正则化自带特征选择的功能，这是由于L1正则化易得稀疏解导致的，因为稀疏解$\boldsymbol w^*$的某些维度$w_i=0$，从而达到了特征选择的功能；</li>
<li>L1正则化的解不稳定，也即可能会有多个解，这是因为L1正则化的红色等高线容易与经验风险的蓝色等高线产生多个切点，例如上图中的蓝色等高线若不为圆形曲线，而是直线时，此时极有可能与L1正则化的红色等高线重合，从而产生多个解；</li>
<li>L1正则化不易求解，这是因为绝对值函数通常都不好求解；</li>
<li>L1正则化相对于L2正则化对异常值敏感度低，这是因为当$\vert w_i \vert&gt;1$时，$\sum\vert w_i \vert &lt; \sum(w_i)^2$，从而对异常值敏感度低。</li>
</ul>
<h3 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h3><p><span id="ref1">[1] 李航.《统计学习方法》</span><br><span id="ref2">[2] <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)" target="_blank" rel="external">Lasso (statistics)</a></span><br><span id="ref3">[3] <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization" target="_blank" rel="external"> Ridge regression</a></span><br><span id="ref4">[4] <a href="https://www.zhihu.com/question/37096933" target="_blank" rel="external">l1 相比于 l2 为什么容易获得稀疏解？</a></span></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项，正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。例如模型参数向量的范数，正则化的一般形式如下&lt;sup&gt;&lt;a href=&quot;#ref1&quot;&gt;[1]&lt;/a&gt;&lt;/sup&gt;：&lt;/p&gt;
&lt;scrip
    
    </summary>
    
      <category term="机器学习" scheme="http://www.sm1les.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="正则化" scheme="http://www.sm1les.com/tags/%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Logistic回归——为什么用对数损失函数而不用平方损失函数？</title>
    <link href="http://www.sm1les.com/2019/01/02/logistic-regression-log-loss-and-quadratic-loss/"/>
    <id>http://www.sm1les.com/2019/01/02/logistic-regression-log-loss-and-quadratic-loss/</id>
    <published>2019-01-02T10:14:17.000Z</published>
    <updated>2019-01-09T07:28:35.391Z</updated>
    
    <content type="html"><![CDATA[<p>损失函数本身作为选取最优模型的准则，当然是可以任意选取的，但是不同的模型选取场景下，各个准则的评价效果是不同的，因此需要因地制宜，选择评价效果最好的准则。</p>
<h3 id="平方损失函数"><a href="#平方损失函数" class="headerlink" title="平方损失函数"></a>平方损失函数</h3><p>由最小二乘法可以推得Logistic回归的平方损失函数为<sup><a href="#ref1">[1]</a></sup>：</p>
<script type="math/tex; mode=display">J(\boldsymbol w)=\sum_{i=1}^{m} \left( y_i - \cfrac{1}{1+e^{-\boldsymbol w^T\boldsymbol x_i}} \right)^2</script><p>此函数为<strong>非凸</strong>函数，易得局部最优解，不易求得全局最优解。</p>
<h3 id="对数损失函数"><a href="#对数损失函数" class="headerlink" title="对数损失函数"></a>对数损失函数</h3><p>对数损失函数也称为对数似然损失函数，一般由最（极）大似然估计法推得，推导过程如下：<br>设$y\in\{0,1\}$，$y$取1和0的概率分别分为$p_1(\boldsymbol{x};\boldsymbol w)$和$p_0(\boldsymbol{x};\boldsymbol w)$，则：</p>
<script type="math/tex; mode=display">\begin{aligned}    
p_1(\boldsymbol{x};\boldsymbol w) &= \cfrac{e^{\boldsymbol w^T\boldsymbol x}}{1+e^{\boldsymbol w^T\boldsymbol x}} \\
p_0(\boldsymbol{x};\boldsymbol w) &= 1-p_1(\boldsymbol{x};\boldsymbol w)=\cfrac{1}{1+e^{\boldsymbol w^T\boldsymbol x}}
\end{aligned}</script><p>似然项为：</p>
<script type="math/tex; mode=display">p(y|\boldsymbol x;\boldsymbol w)=[p_1(\boldsymbol{x};\boldsymbol w)]^{y}[p_0(\boldsymbol{x};\boldsymbol w)]^{1-y}</script><p>对数似然函数为：</p>
<script type="math/tex; mode=display">\begin{aligned}    
\ln L(\boldsymbol w) &= \sum_{i=1}^{m}\left(y_i\ln(p_1(\boldsymbol{x_i};\boldsymbol w))+(1-y_i)\ln(p_0(\boldsymbol{x_i};\boldsymbol w)) \right) \\
&= \sum_{i=1}^{m}\left(y_i \boldsymbol w^T\boldsymbol x_i-\ln(1+e^{\boldsymbol w^T\boldsymbol x_i}) \right) 
\end{aligned}</script><p>似然函数的目标是最大化函数值，而损失函数的目标是最小化函数值，所以将上式添加一个负号即可得对数损失函数：</p>
<script type="math/tex; mode=display">J(\boldsymbol w) = \sum_{i=1}^{m}\left(-y_i \boldsymbol w^T\boldsymbol x_i+\ln(1+e^{\boldsymbol w^T\boldsymbol x_i}) \right)</script><p>此函数为高阶连续可导凸函数，可以用各种凸优化算法求得全局最优解，这就是为什么用对数损失函数而不用平方损失函数的原因。<br>【注】：</p>
<ul>
<li>$y\in\{0,1\}$时两种似然项构造：<script type="math/tex; mode=display">\begin{aligned}
p(y|\boldsymbol x;\boldsymbol w) &= [p_1(\boldsymbol{x};\boldsymbol w)]^{y}[p_0(\boldsymbol{x};\boldsymbol w)]^{1-y} \\
p(y|\boldsymbol x;\boldsymbol w) &= yp_1(\boldsymbol{x};\boldsymbol w)+(1-y)p_0(\boldsymbol{x};\boldsymbol w)
\end{aligned}</script></li>
<li>$y\in\{1,-1\}$时一种似然项构造：<script type="math/tex; mode=display">p(y|\boldsymbol x;\boldsymbol w)=\cfrac{1}{1+e^{-y\boldsymbol w^T\boldsymbol x}}</script>其相应的对数似然函数为：<script type="math/tex; mode=display">\ln L(\boldsymbol w)=-\sum_{i=1}^{m}\ln(1+e^{-y_i\boldsymbol w^T\boldsymbol x_i})</script></li>
</ul>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p><span id="ref1">[1] <a href="https://www.zhihu.com/question/65350200" target="_blank" rel="external">逻辑回归损失函数为什么使用最大似然估计而不用最小二乘法？</a></span></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;损失函数本身作为选取最优模型的准则，当然是可以任意选取的，但是不同的模型选取场景下，各个准则的评价效果是不同的，因此需要因地制宜，选择评价效果最好的准则。&lt;/p&gt;
&lt;h3 id=&quot;平方损失函数&quot;&gt;&lt;a href=&quot;#平方损失函数&quot; class=&quot;headerlink&quot; ti
    
    </summary>
    
      <category term="机器学习" scheme="http://www.sm1les.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Logistic回归" scheme="http://www.sm1les.com/tags/Logistic%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>Logistic回归——sigmoid函数的由来</title>
    <link href="http://www.sm1les.com/2018/12/19/logistic-regression-the-origin-of-sigmoid/"/>
    <id>http://www.sm1les.com/2018/12/19/logistic-regression-the-origin-of-sigmoid/</id>
    <published>2018-12-19T11:14:37.000Z</published>
    <updated>2019-01-09T07:29:29.460Z</updated>
    
    <content type="html"><![CDATA[<p>Logistic回归是一种广义线性模型（Generalized Linear Models），在学习广义线性模型之前需要先学习一下指数族分布（Exponential Family Distributions）。</p>
<h3 id="指数族分布"><a href="#指数族分布" class="headerlink" title="指数族分布"></a>指数族分布</h3><p>顾名思义，指数族分布是某一类分布的总称，其分布律（概率密度）的一般表达形式如下：</p>
<script type="math/tex; mode=display">p(y;\eta) = b(y) \exp(\eta^T T(y) − a(\eta)) \label{1} \qquad (A.1)</script><p>其中$\eta$为natural parameter (也称canonical parameter)；$T(y)$为sufficient statistic（通常取$T(y)=y$）；$a(\eta)$是用来保证分布律的累加和$\sum p(y;\eta)=1$或者概率密度的积分值$\int p(y;\eta)=1$；$b(y)$为一个关于$y$的函数。<strong>伯努利分布</strong>和<strong>高斯分布</strong>均属于指数族分布，推导过程如下：</p>
<h5 id="伯努利分布："><a href="#伯努利分布：" class="headerlink" title="伯努利分布："></a>伯努利分布：</h5><script type="math/tex; mode=display">p(y)=\phi^y (1-\phi)^{1-y}</script><p>其中$y=0,1$，$\phi$为$y=1$的概率$p(y=1)$。对其进行恒等变形可得：</p>
<script type="math/tex; mode=display">
\begin{aligned}    
p(y) &= \phi^y (1-\phi)^{1-y} \\
&= \exp(y \ln \phi+(1−y) \ln(1−\phi)) \\
&= \exp \left((\ln(\cfrac{\phi}{1-\phi}))y+\ln(1-\phi) \right)
\end{aligned}</script><p>对照式(A.1)可知，伯努利分布的指数族分布参数为：</p>
<script type="math/tex; mode=display">
\begin{aligned}    
\eta &= \ln(\cfrac{\phi}{1-\phi}) &(B.1)\\
T(y) &= y &(B.2)\\
a(\eta) &= -\ln(1-\phi) &(B.3)\\
b(y) &= 1 &(B.4)
\end{aligned}</script><h5 id="高斯分布："><a href="#高斯分布：" class="headerlink" title="高斯分布："></a>高斯分布：</h5><p>参见<a href="#ref1">[1]</a></p>
<h3 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a>广义线性模型</h3><p>由以下三条假设构建出的模型即为广义线性模型：</p>
<ol>
<li>给定$\boldsymbol x$和$\theta$，$y$服从参数为$\eta$的指数族分布，即$y | \boldsymbol x; \theta \sim ExponentialFamily(\eta)$；</li>
<li>在给定$\boldsymbol x$的前提下，模型的目标是求出用$\boldsymbol x$表示的$T(y)$的<strong>期望值</strong>表达式，即$h_{\theta}(\boldsymbol x)=E(T(y)|\boldsymbol x;\theta)$；</li>
<li>参数$\eta$和$\boldsymbol x$呈线性关系，即$\eta = \theta^T \boldsymbol x$.</li>
</ol>
<p>在对二分类任务进行建模的时候，类别$y ∈ \{0,1\}$，所以我们很自然地选择伯努利分布来对$y$进行建模，即假设$y$服从伯努利分布，接下来便可以用上述三条假设和伯努利分布的指数族分布参数来构建一个广义线性模型：</p>
<script type="math/tex; mode=display">
\begin{aligned}    
h(\boldsymbol x) &= E(T(y)|\boldsymbol x;\theta) \\
&= E(y|\boldsymbol x;\theta) \\
&= \phi \\
&= \cfrac{1}{1+e^{(-\eta)}} \\
&= \cfrac{1}{1+e^{(-\theta^T\boldsymbol x)}}
\end{aligned}</script><p>第1个等式即为假设2；第2个等式由式(B.2)导出；第3个等式是因为伯努利分布的期望为$E(y|\boldsymbol x;\theta)=1· \phi + 0 · (1-\phi)=\phi$；第4个等式是由式(B.1)导出；第5个等式则是由假设3导出。此广义线性模型即为Logistic回归模型，在给定$\boldsymbol x$和$\theta$时，$y=1$的概率$p(y=1|\boldsymbol x;\theta)=\phi=\cfrac{1}{1+e^{(-\theta^T\boldsymbol x)}}$即为sigmoid函数。<br>【注】：</p>
<ul>
<li>上述三条假设是一种固定的广义线性模型构建方法，在问题建模的过程中我们唯一要做的就是假设$y$服从何种<strong>指数族分布</strong>，例如在做二分类问题时通常假设$y$服从伯努利分布，在做回归问题时通常假设$y$服从高斯分布，在做网站访问量预测时通常假设$y$服从泊松分布。在确定$y$的分布后，只需按照上述三条假设即可构建出一个广义线性模型；</li>
<li>线性回归也属于广义线性模型，推导过程参见<a href="#ref1">[1]</a>。</li>
</ul>
<h3 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h3><p><span id="ref1">[1] cs229-notes1</span></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Logistic回归是一种广义线性模型（Generalized Linear Models），在学习广义线性模型之前需要先学习一下指数族分布（Exponential Family Distributions）。&lt;/p&gt;
&lt;h3 id=&quot;指数族分布&quot;&gt;&lt;a href=&quot;#指数
    
    </summary>
    
      <category term="机器学习" scheme="http://www.sm1les.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Logistic回归" scheme="http://www.sm1les.com/tags/Logistic%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>拉格朗日对偶性</title>
    <link href="http://www.sm1les.com/2018/12/11/lagrange-duality/"/>
    <id>http://www.sm1les.com/2018/12/11/lagrange-duality/</id>
    <published>2018-12-11T07:36:48.000Z</published>
    <updated>2019-01-05T10:43:43.769Z</updated>
    
    <content type="html"><![CDATA[<p>对于带不等式约束的优化问题：</p>
<script type="math/tex; mode=display">\begin{aligned}
\min \quad f(\boldsymbol x) \\
s.t.  \quad g_i(\boldsymbol x) &\leq 0 \quad (i=1,...,n) \\
h_j(\boldsymbol x) &= 0 \quad (j=1,...,m)
\end{aligned}</script><p>令上述优化问题为<strong>“主问题”</strong>，其拉格朗日函数为：</p>
<script type="math/tex; mode=display">L(\boldsymbol x,\boldsymbol \mu,\boldsymbol \lambda)=f(\boldsymbol x)+\sum_{i=1}^{n}\mu_i g_i(\boldsymbol x)+\sum_{j=1}^{m}\lambda_j h_j(\boldsymbol x)</script><p>其中$\boldsymbol \mu=(\mu_1,\mu_2,…,\mu_n)^T,\boldsymbol \lambda=(\lambda_1,\lambda_2,…,\lambda_m)^T$<br>定义$L(\boldsymbol x,\boldsymbol \mu,\boldsymbol \lambda)$的<strong>对偶函数</strong>为：</p>
<script type="math/tex; mode=display">\Gamma (\boldsymbol \mu,\boldsymbol \lambda)=\mathop{\inf}_{\boldsymbol x∈D}L(\boldsymbol x,\boldsymbol \mu,\boldsymbol \lambda)=\mathop{\inf}_{\boldsymbol x∈D} \left ( f(\boldsymbol x)+\sum_{i=1}^{n}\mu_i g_i(\boldsymbol x)+\sum_{j=1}^{m}\lambda_j h_j(\boldsymbol x) \right)</script><p>其中$D=\boldsymbol{dom}  f \cap \bigcap\limits_{i=1}^{m}\boldsymbol{dom}  g_i \cap \bigcap\limits_{j=1}^{n}\boldsymbol{dom}  h_j $<br>设$\tilde{\boldsymbol x}$为可行域中的点（即$g_i(\tilde{\boldsymbol x}) \leq 0$且$h_j(\tilde{\boldsymbol x}) = 0$），当$\boldsymbol \mu \succeq 0$时：</p>
<script type="math/tex; mode=display">\sum_{i=1}^{n}\mu_i g_i(\tilde{\boldsymbol x})+\sum_{j=1}^{m}\lambda_j h_j(\tilde{\boldsymbol x}) \leq 0</script><p>则</p>
<script type="math/tex; mode=display">\Gamma (\boldsymbol \mu,\boldsymbol \lambda)=\mathop{\inf}_{\boldsymbol x∈D}L(\boldsymbol x,\boldsymbol \mu,\boldsymbol \lambda) \leq L(\tilde{\boldsymbol x},\boldsymbol \mu,\boldsymbol \lambda) \leq f(\tilde{\boldsymbol x})</script><script type="math/tex; mode=display">\Gamma (\boldsymbol \mu,\boldsymbol \lambda) \leq f(\tilde{\boldsymbol x})</script><p>这意味着当$\boldsymbol \mu \succeq 0$时，可行域中每个点$\tilde{\boldsymbol x}$均满足$\Gamma (\boldsymbol \mu,\boldsymbol \lambda)\leq f(\tilde{\boldsymbol x})$，设主问题的最优值为$p^*$（即$\min f(\tilde{\boldsymbol x})$），则当$\boldsymbol \mu \succeq 0$时，$\Gamma (\boldsymbol \mu,\boldsymbol \lambda) \leq p^*$恒成立，此时对偶函数构成了最优值$p^*$的下界。于是可以考虑求最优下界来逼近$p^*$，也就是求对偶函数<strong>在$\boldsymbol \mu \succeq 0$约束下</strong>的最大值：</p>
<script type="math/tex; mode=display">\max \Gamma (\boldsymbol \mu,\boldsymbol \lambda) \quad  s.t. \ \boldsymbol \mu \succeq 0</script><p>此时求对偶函数最大值的问题便是主问题的<strong>“对偶问题”</strong>。设对偶问题的最优值为$d^*$，显然$d^* \leq p^*$，此时称为“弱对偶性”成立，若$d^* = p^*$，则称为“强对偶性”成立，此时对偶问题的最优值便是主问题的最优值。<br>【注】：</p>
<ul>
<li>对偶问题恒为<strong>凸优化问题</strong>，因为对偶函数$\Gamma (\boldsymbol \mu,\boldsymbol \lambda)$是一族关于$(\boldsymbol \mu,\boldsymbol \lambda)$的仿射函数的逐点下确界，恒为<strong>凹函数</strong>（加个负号即可转为凸函数）（证明参见<a href="#ref1">[1]</a> § 3.2.3），约束条件$\boldsymbol \mu \succeq 0$为凸集，且与主问题的凹凸性无关。这也就是为什么不直接求解主问题转而去求解对偶问题的主要原因；</li>
<li>当主问题为<strong>凸优化问题</strong>时，若满足某些<strong>约束限制条件（constraint qualification）</strong>则强对偶性成立。常见的约束限制条件有<strong>Slater条件</strong><sup><a href="#ref2">[2]</a></sup>：“当可行域中存在一点在使得等式约束成立的同时也使得<strong>所有</strong>不等式约束<strong>严格成立</strong>（即$\exists  h_j(\tilde{\boldsymbol x}_a) = 0,g_i(\tilde{\boldsymbol x}_a) &lt; 0$）时，强对偶性成立。”（证明参见<a href="#ref1">[1]</a> § 5.3.2）Slater条件仅是众多使得凸优化问题强对偶性成立的约束限制条件之一，而且它也是<a href="https://www.sm1les.com/2018/12/08/karush-kuhn-tucker-conditions">KKT条件</a>中的约束限制条件之一；</li>
<li>Slater条件的“弱化”形式：“若前k个不等式约束为<strong>仿射函数</strong>，当$\exists  h_j(\tilde{\boldsymbol x}_a) = 0, g_i(\tilde{\boldsymbol x}_a) \leq 0  (i=1,…k),g_i(\tilde{\boldsymbol x}_a) &lt; 0  (i=k+1,…m)$时，凸优化问题强对偶性成立。”也就是说当不等式约束中有仿射函数时，不要求可行域中存在一点使得这些仿射不等式严格成立。<strong>特别地</strong>，若所有约束条件都是仿射等式或不等式，且$\boldsymbol{dom}  f$为开集，则凸优化问题在其可行域内强对偶性成立，例如SVM、任何可行的线性规划问题；</li>
<li>设$\boldsymbol x^*$是可行域中的最优解（即$f(\boldsymbol x^*)=p^*$），若$f(\boldsymbol x),g_i(\boldsymbol x),h_j(\boldsymbol x)$<strong>一阶可微</strong>，且强对偶性成立（不一定通过Slater条件得到），则$\boldsymbol x^*$一定满足KKT条件（证明参见<a href="#ref1">[1]</a> § 5.5.2-§ 5.5.3，其中§ 5.5.2里“$L(\boldsymbol x,\boldsymbol \mu^*,\boldsymbol \lambda^*)$关于$\boldsymbol x$求极小时在$\boldsymbol x^*$处取得最小值”的原因是主问题的最小值点必是拉格朗日函数的极值点之一）；</li>
<li>若主问题为<strong>凸优化问题</strong>，且$f(\boldsymbol x),g_i(\boldsymbol x),h_j(\boldsymbol x)$<strong>一阶可微</strong>，存在$\boldsymbol x^*,\boldsymbol \mu^*,\boldsymbol \lambda^*$满足KKT条件（即$\boldsymbol x^*$满足约束限制条件），那么$\boldsymbol x^*$和$(\boldsymbol \mu^*,\boldsymbol \lambda^*)$即为主问题和对偶问题的最优解，强对偶性成立（即$p^*=f(\boldsymbol x^*)=\Gamma (\boldsymbol \mu^*,\boldsymbol \lambda^*)=d^*$）（证明参见<a href="#ref1">[1]</a> § 5.5.3），显然此时KKT条件为最优解的充要条件，完全可以通过直接求满足KKT条件的点来求解最优解；</li>
<li>KKT条件成立和强对偶性成立没有必然的联系，也就是说当KKT条件成立时<strong>不一定</strong>能推出强对偶性成立，通常强对偶性成立的条件更为苛刻。</li>
<li>若拉格朗日函数$L(\boldsymbol x,\boldsymbol \mu,\boldsymbol \lambda)$是<strong>关于$\boldsymbol x$的一阶可微凸函数</strong>，则其对偶函数的表达式求解过程如下：先对$L(\boldsymbol x,\boldsymbol \mu,\boldsymbol \lambda)$关于$\boldsymbol x$求导并令导数等于0，得到一个$\boldsymbol x$关于$\boldsymbol \mu$和$\boldsymbol \lambda$的表达式$\boldsymbol x=\Phi (\boldsymbol \mu,\boldsymbol \lambda)$，然后将其带回到$L(\boldsymbol x,\boldsymbol \mu,\boldsymbol \lambda)$中即可得仅含$\boldsymbol \mu$和$\boldsymbol \lambda$的对偶函数$L \left ( \Phi (\boldsymbol \mu,\boldsymbol \lambda),\boldsymbol \mu,\boldsymbol \lambda \right ) =\Gamma (\boldsymbol \mu,\boldsymbol \lambda)$，例<a href="#ref1">[1]</a> § 5.2.4和SVM的对偶函数求解。</li>
</ul>
<h3 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h3><p><span id="ref1">[1] 王书宁 译.《凸优化》</span><br><span id="ref2">[2] <a href="https://en.wikipedia.org/wiki/Slater%27s_condition" target="_blank" rel="external">Slater’s condition</a></span></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于带不等式约束的优化问题：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\min \quad f(\boldsymbol x) \\
s.t.  \quad g_i(\boldsymbol x) &amp;\
    
    </summary>
    
      <category term="最优化" scheme="http://www.sm1les.com/categories/%E6%9C%80%E4%BC%98%E5%8C%96/"/>
    
    
      <category term="对偶问题" scheme="http://www.sm1les.com/tags/%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98/"/>
    
      <category term="Slater条件" scheme="http://www.sm1les.com/tags/Slater%E6%9D%A1%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>Karush–Kuhn–Tucker（简称KKT）条件</title>
    <link href="http://www.sm1les.com/2018/12/08/karush-kuhn-tucker-conditions/"/>
    <id>http://www.sm1les.com/2018/12/08/karush-kuhn-tucker-conditions/</id>
    <published>2018-12-08T09:03:27.000Z</published>
    <updated>2019-01-05T10:44:02.046Z</updated>
    
    <content type="html"><![CDATA[<p>对于带不等式约束的优化问题：</p>
<script type="math/tex; mode=display">\begin{aligned}
\min \quad f(\boldsymbol x) \\
s.t.  \quad g_i(\boldsymbol x) &\leq 0 \quad (i=1,...,n) \\
h_j(\boldsymbol x) &= 0 \quad (j=1,...,m)
\end{aligned}</script><p>若$f(\boldsymbol x),g_i(\boldsymbol x),h_j(\boldsymbol x)$在$\boldsymbol x^*$处<strong>一阶可微</strong>，$\boldsymbol x^*$是优化问题的局部可行解（此时为极小值解），并且在$\boldsymbol x^*$处<strong>约束限制条件（constraint qualifications）</strong><sup><a href="#ref1">[1]</a></sup><sup><a href="#ref2">[2]</a></sup>成立，则存在$\boldsymbol \mu^*=(\mu_1^*,\mu_2^*,…,\mu_n^*)^T,\boldsymbol \lambda^*=(\lambda_1^*,\lambda_2^*,…,\lambda_m^*)^T,$使得：</p>
<script type="math/tex; mode=display">\left\{
\begin{aligned}
& \nabla L(\boldsymbol x^* ,\boldsymbol \mu^* ,\boldsymbol \lambda^* )=\nabla f(\boldsymbol  x^* )+\sum_{i=1}^{n}\mu_i^* \nabla g_i(\boldsymbol x^* )+\sum_{j=1}^{m}\lambda_j^* \nabla h_j(\boldsymbol x^*) &(1) \\
& h_j(\boldsymbol x^*)=0 &(2) \\
& g_i(\boldsymbol x^*) \leq 0 &(3) \\
& \mu_i^* \geq 0 &(4)\\
& \mu_i^* g_i(\boldsymbol x^*)=0 &(5)
\end{aligned}
\right.</script><p>其中$L(\boldsymbol x,\boldsymbol \mu,\boldsymbol \lambda)$为拉格朗日函数：</p>
<script type="math/tex; mode=display">L(\boldsymbol x,\boldsymbol \mu,\boldsymbol \lambda)=f(\boldsymbol x)+\sum_{i=1}^{n}\mu_i g_i(\boldsymbol x)+\sum_{j=1}^{m}\lambda_j h_j(\boldsymbol x)</script><p>以上即5条为KKT条件，其中条件(2)和(3)为约束条件显然成立，条件(1)、(4)、(5)成立的<strong>简单证明</strong>如下：</p>
<p><center>
<img src="./Inequality_constraint_diagram.png" width="60%" height="60%">
</center><br>由约束条件：$g_i(\boldsymbol x) \leq 0$可知，$\boldsymbol x^*$一定满足$g_i(\boldsymbol x^*) \lt 0$或者$g_i(\boldsymbol x^*) = 0$（也即条件(3)），对这两种情形进行讨论：</p>
<ul>
<li><strong>当$g_i(\boldsymbol x^*) \lt 0$时（如上左图）：</strong><br>此时极小值点在$g_i(\boldsymbol x) \lt 0$这个区域内，等价于<strong>无不等式约束条件</strong>的最优化问题，所以$\mu_i^* = 0$。</li>
<li><strong>当$g_i(\boldsymbol x^*) = 0$时（如上右图）：</strong><br>此时极小值点在$g_i(\boldsymbol x) = 0$这个边界上，等价于<strong>仅含等式约束条件</strong>的最优化问题，所以根据拉格朗日乘子法的原理可知，此时$f(\boldsymbol  x)$和$ g_i(\boldsymbol  x)=0$在极小值点处相切，梯度（正梯度）平行，即$\nabla f(\boldsymbol  x^*)+\mu_i^* \nabla g_i(\boldsymbol  x^*)=0 \quad (\mu_i^*∈\boldsymbol R)$。易知$g_i(\boldsymbol x)=0$这个边界上的梯度都是向外的，所以$\nabla f(\boldsymbol x^*)$必然和$\nabla g_i(\boldsymbol x^*)$的方向相反（如果不相反的话极小值点就不会在边界上取到而是在$g_i(\boldsymbol x) \lt 0$内取），所以$\nabla f(\boldsymbol  x^*)$和$\nabla g_i(\boldsymbol  x^*)$异号，也即$\mu_i^* \gt 0$。</li>
</ul>
<p>综上，条件(4)：$\mu_i^* \geq 0$恒成立；当$g_i(\boldsymbol x^*) \lt 0$时$\mu_i^* = 0$， $g_i(\boldsymbol x^*) = 0$时$\mu_i^* \gt 0$，所以条件(5)：$\mu_i^* g_i(\boldsymbol x^*)=0$恒成立；$\lambda_j$为拉格朗日乘子，由拉格朗日乘子法的原理可知，$\lambda_j$取值无要求，所以总存在$\lambda_j^*$使得条件(1)恒成立。<br>【注】：</p>
<ul>
<li>若局部可行解$\boldsymbol x^*$不满足<strong>约束限制条件（constraint qualifications）</strong><sup><a href="#ref1">[1]</a></sup><sup><a href="#ref2">[2]</a></sup>，则<strong>不一定</strong>满足KKT条件，也就是说约束限制条件仅是充分条件，而不是充要条件，例如上述优化问题强对偶性成立时（不一定通过Slater条件得到），$\boldsymbol x^*$也满足KKT条件；</li>
<li>KKT条件是局部可行解$\boldsymbol x^*$的一个<strong>必要条件</strong>；</li>
<li>对于上述优化问题，若目标函数$f(\boldsymbol x)$是凸函数，不等式约束$g_i(\boldsymbol x)$是凸函数，等式约束$h_j(\boldsymbol x)$是仿射函数，则称该优化问题为<strong>凸优化</strong>，此时KKT条件升级为<strong>充要条件</strong>，$\boldsymbol x^*$升级为<strong>全局可行解</strong>；</li>
<li>以上部分论述的严格数学证明参见<a href="#ref1">[1]</a> § 4.2</li>
</ul>
<h3 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h3><p><span id="ref1">[1] 王燕军. 《最优化基础理论与方法》</span><br><span id="ref2">[2] <a href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions#Regularity_conditions_(or_constraint_qualifications)" target="_blank" rel="external">Karush–Kuhn–Tucker conditions</a></span></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于带不等式约束的优化问题：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\min \quad f(\boldsymbol x) \\
s.t.  \quad g_i(\boldsymbol x) &amp;\
    
    </summary>
    
      <category term="最优化" scheme="http://www.sm1les.com/categories/%E6%9C%80%E4%BC%98%E5%8C%96/"/>
    
    
      <category term="KKT条件" scheme="http://www.sm1les.com/tags/KKT%E6%9D%A1%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>线性回归——最（极）大似然估计和最小二乘法推导</title>
    <link href="http://www.sm1les.com/2018/11/27/linear-regression-maximum-likelihood/"/>
    <id>http://www.sm1les.com/2018/11/27/linear-regression-maximum-likelihood/</id>
    <published>2018-11-27T07:20:01.000Z</published>
    <updated>2019-01-09T07:28:50.727Z</updated>
    
    <content type="html"><![CDATA[<h3 id="最（极）大似然估计推导："><a href="#最（极）大似然估计推导：" class="headerlink" title="最（极）大似然估计推导："></a>最（极）大似然估计推导：</h3><p>对于线性回归模型：</p>
<script type="math/tex; mode=display">y=\boldsymbol{w}^T\boldsymbol{x}+b+\epsilon</script><p>随机误差$\epsilon$可以看成是由许多观察不到的、可加的微小误差叠加而成的<sup><a href="#ref1">[1]</a></sup>，则根据<strong>中心极限定理</strong>，随机误差$\epsilon$服从正态分布，其分布为：$\epsilon \sim N(\mu,\sigma^2)$，为了方便后续计算可以将其去均值，令$\epsilon:=\epsilon-\mu$，则此时的分布为：$\epsilon \sim N(0,\sigma^2)$，分布律为：</p>
<script type="math/tex; mode=display">p(\epsilon)=\cfrac{1}{\sqrt{2\pi}\sigma} \mathrm{exp}\left(-\cfrac{\epsilon^2}{2\sigma^2}\right)</script><p>若将其中的$\epsilon$用$\epsilon=y-\boldsymbol{w}^T\boldsymbol{x}-b$等价替换可得：</p>
<script type="math/tex; mode=display">p(y)=\cfrac{1}{\sqrt{2\pi}\sigma} \mathrm{exp}\left(-\cfrac{(y-\boldsymbol{w}^T\boldsymbol{x}-b)^2}{2\sigma^2}\right)</script><p>上式显然可以看做$y \sim N(\boldsymbol{w}^T\boldsymbol{x}+b,\sigma^2)$的分布律，接下来便可以用最（极）大似然估计法来估计$\boldsymbol{w}$和b的值。似然函数为：</p>
<script type="math/tex; mode=display">L(\boldsymbol{w},b)=\prod_{i=1}^{m}p(y_i)=\prod_{i=1}^{m}\cfrac{1}{\sqrt{2\pi}\sigma} \mathrm{exp}\left(-\cfrac{(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2}{2\sigma^2}\right)</script><p>两边同时取对数得对数似然函数：</p>
<script type="math/tex; mode=display">
\begin{aligned}     
\ln L(\boldsymbol{w},b) & = \sum_{i=1}^{m}\ln \cfrac{1}{\sqrt{2\pi}\sigma} \mathrm{exp}\left(-\cfrac{(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2}{2\sigma^2}\right) \\
& = \sum_{i=1}^{m}\ln \cfrac{1}{\sqrt{2\pi}\sigma}+\sum_{i=1}^{m}\ln \mathrm{exp}\left(-\cfrac{(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2}{2\sigma^2}\right) \\
& = m\ln \cfrac{1}{\sqrt{2\pi}\sigma}-\cfrac{1}{2\sigma^2}\sum_{i=1}^{m}(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2
\end{aligned}</script><p>其中$m$，$\sigma$均为常数，所以最大化$\ln L(\boldsymbol{w},b)$等价于最小化$\sum_{i=1}^{m}(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2$，也即：</p>
<script type="math/tex; mode=display">(\boldsymbol{w}',b')=\mathop{\arg\max}_{(\boldsymbol{w},b)} \ln L(\boldsymbol{w},b)=\mathop{\arg\min}_{(\boldsymbol{w},b)} \sum_{i=1}^{m}(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2</script><p>其中$\boldsymbol{w}’$和$b’$为$\boldsymbol{w}$和$b$的解.</p>
<h3 id="最小二乘法推导："><a href="#最小二乘法推导：" class="headerlink" title="最小二乘法推导："></a>最小二乘法推导：</h3><p>最小二乘法是基于均方误差最小化来对模型进行参数估计的，用公式表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned}    
    (\boldsymbol{w}',b') & = \mathop{\arg\min}_{(\boldsymbol{w},b)} \sum_{i=1}^{m}(y_i-(\boldsymbol{w}^T\boldsymbol{x_i}+b))^2 \\
    & = \mathop{\arg\min}_{(\boldsymbol{w},b)} \sum_{i=1}^{m}(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2
\end{aligned}</script><p>显然与最（极）大似然估计的推导结果一致。</p>
<h3 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h3><p><span id="ref1">[1] 盛骤.《概率论与数理统计（第四版）》</span></p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;最（极）大似然估计推导：&quot;&gt;&lt;a href=&quot;#最（极）大似然估计推导：&quot; class=&quot;headerlink&quot; title=&quot;最（极）大似然估计推导：&quot;&gt;&lt;/a&gt;最（极）大似然估计推导：&lt;/h3&gt;&lt;p&gt;对于线性回归模型：&lt;/p&gt;
&lt;script type=&quot;ma
    
    </summary>
    
      <category term="机器学习" scheme="http://www.sm1les.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="线性回归" scheme="http://www.sm1les.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
</feed>
