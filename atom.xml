<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Sm1les&#39;s blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.sm1les.com/"/>
  <updated>2018-12-13T13:13:24.023Z</updated>
  <id>http://www.sm1les.com/</id>
  
  <author>
    <name>Sm1les</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>拉格朗日对偶性</title>
    <link href="http://www.sm1les.com/2018/12/11/lagrange-duality/"/>
    <id>http://www.sm1les.com/2018/12/11/lagrange-duality/</id>
    <published>2018-12-11T07:36:48.000Z</published>
    <updated>2018-12-13T13:13:24.023Z</updated>
    
    <content type="html"><![CDATA[<p>对于带不等式约束的优化问题：</p>
<script type="math/tex; mode=display">\begin{aligned}
\min \quad f(\boldsymbol x) \\
s.t.  \quad g_i(\boldsymbol x) &\leq 0 \quad (i=1,...,n) \\
h_j(\boldsymbol x) &= 0 \quad (j=1,...,m)
\end{aligned}</script><p>令上述优化问题为<strong>“主问题”</strong>，其拉格朗日函数为：</p>
<script type="math/tex; mode=display">L(\boldsymbol x,\boldsymbol \mu,\boldsymbol \lambda)=f(\boldsymbol x)+\sum_{i=1}^{n}\mu_i g_i(\boldsymbol x)+\sum_{j=1}^{m}\lambda_j h_j(\boldsymbol x)</script><p>其中$\boldsymbol \mu=(\mu_1,\mu_2,…,\mu_n)^T,\boldsymbol \lambda=(\lambda_1,\lambda_2,…,\lambda_m)^T$<br>定义$L(\boldsymbol x,\boldsymbol \mu,\boldsymbol \lambda)$的<strong>对偶函数</strong>为：</p>
<script type="math/tex; mode=display">\Gamma (\boldsymbol \mu,\boldsymbol \lambda)=\mathop{\inf}_{\boldsymbol x∈D}L(\boldsymbol x,\boldsymbol \mu,\boldsymbol \lambda)=\mathop{\inf}_{\boldsymbol x∈D} \left ( f(\boldsymbol x)+\sum_{i=1}^{n}\mu_i g_i(\boldsymbol x)+\sum_{j=1}^{m}\lambda_j h_j(\boldsymbol x) \right)</script><p>其中$D=\boldsymbol{dom}  f \cap \bigcap\limits_{i=1}^{m}\boldsymbol{dom}  g_i \cap \bigcap\limits_{j=1}^{n}\boldsymbol{dom}  h_j $<br>设$\tilde{\boldsymbol x}$为可行域中的点（即$g_i(\tilde{\boldsymbol x}) \leq 0$且$h_j(\tilde{\boldsymbol x}) = 0$），当$\boldsymbol \mu \succeq 0$时：</p>
<script type="math/tex; mode=display">\sum_{i=1}^{n}\mu_i g_i(\tilde{\boldsymbol x})+\sum_{j=1}^{m}\lambda_j h_j(\tilde{\boldsymbol x}) \leq 0</script><p>则</p>
<script type="math/tex; mode=display">\Gamma (\boldsymbol \mu,\boldsymbol \lambda)=\mathop{\inf}_{\boldsymbol x∈D}L(\boldsymbol x,\boldsymbol \mu,\boldsymbol \lambda) \leq L(\tilde{\boldsymbol x},\boldsymbol \mu,\boldsymbol \lambda) \leq f(\tilde{\boldsymbol x})</script><script type="math/tex; mode=display">\Gamma (\boldsymbol \mu,\boldsymbol \lambda) \leq f(\tilde{\boldsymbol x})</script><p>这意味着当$\boldsymbol \mu \succeq 0$时，可行域中每个点$\tilde{\boldsymbol x}$均满足$\Gamma (\boldsymbol \mu,\boldsymbol \lambda)\leq f(\tilde{\boldsymbol x})$，设主问题的最优值为$p^*$（即$\min f(\tilde{\boldsymbol x})$），则当$\boldsymbol \mu \succeq 0$时，$\Gamma (\boldsymbol \mu,\boldsymbol \lambda) \leq p^*$恒成立，此时对偶函数构成了最优值$p^*$的下界。于是可以考虑求最优下界，也就是求对偶函数<strong>在$\boldsymbol \mu \succeq 0$约束下</strong>的最大值：</p>
<script type="math/tex; mode=display">\max \Gamma (\boldsymbol \mu,\boldsymbol \lambda) \quad  s.t. \ \boldsymbol \mu \succeq 0</script><p>此时求对偶函数最大值的问题便是主问题的<strong>“对偶问题”</strong>。设对偶问题的最优值为$d^*$，显然$d^* \leq p^*$，此时称为“弱对偶性”成立，若$d^* = p^*$，则称为“强对偶性”成立，此时对偶问题的最优值便是主问题的最优值。<br>【注】：</p>
<ul>
<li>对偶问题恒为凸优化问题，因为对偶函数$\Gamma (\boldsymbol \mu,\boldsymbol \lambda)$是一族关于$(\boldsymbol \mu,\boldsymbol \lambda)$的仿射函数的逐点下确界，恒为<strong>凹函数</strong>（加个负号即可转为凸函数），约束条件$\boldsymbol \mu \succeq 0$为凸集，且与主问题的凹凸性无关。这也就是为什么不直接求解主问题转而去求解对偶问题的主要原因；</li>
<li>若主问题为凸优化问题，即目标函数$f(\boldsymbol x)$是凸函数，不等式约束$g_i(\boldsymbol x)$是凸函数，等式约束$h_j(\boldsymbol x)$是仿射函数，且可行域中至少有一点使得不等式约束<strong>严格成立</strong>（即$\exists  g_i(\tilde{\boldsymbol x}) &lt; 0$），则此时强对偶性成立（这称为<a href="https://en.wikipedia.org/wiki/Slater%27s_condition" target="_blank" rel="external">Slater条件</a>）；</li>
<li>若拉格朗日函数$L(\boldsymbol x,\boldsymbol \mu,\boldsymbol \lambda)$是<strong>关于$\boldsymbol x$</strong>的凸函数，则其对偶函数的表达式求解过程如下：先对$L(\boldsymbol x,\boldsymbol \mu,\boldsymbol \lambda)$关于$\boldsymbol x$求导并令导数等于0，得到一个$\boldsymbol x$关于$\boldsymbol \mu$和$\boldsymbol \lambda$的表达式$\boldsymbol x=\Phi (\boldsymbol \mu,\boldsymbol \lambda)$，然后将其带回到$L(\boldsymbol x,\boldsymbol \mu,\boldsymbol \lambda)$中即可得仅含$\boldsymbol \mu$和$\boldsymbol \lambda$的对偶函数$L \left ( \Phi (\boldsymbol \mu,\boldsymbol \lambda),\boldsymbol \mu,\boldsymbol \lambda \right ) =\Gamma (\boldsymbol \mu,\boldsymbol \lambda)$</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于带不等式约束的优化问题：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\min \quad f(\boldsymbol x) \\
s.t.  \quad g_i(\boldsymbol x) &amp;\
    
    </summary>
    
      <category term="最优化" scheme="http://www.sm1les.com/categories/%E6%9C%80%E4%BC%98%E5%8C%96/"/>
    
    
      <category term="对偶问题" scheme="http://www.sm1les.com/tags/%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98/"/>
    
      <category term="Slater条件" scheme="http://www.sm1les.com/tags/Slater%E6%9D%A1%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>Karush–Kuhn–Tucker（简称KKT）条件</title>
    <link href="http://www.sm1les.com/2018/12/08/karush-kuhn-tucker-conditions/"/>
    <id>http://www.sm1les.com/2018/12/08/karush-kuhn-tucker-conditions/</id>
    <published>2018-12-08T09:03:27.000Z</published>
    <updated>2018-12-13T07:50:37.811Z</updated>
    
    <content type="html"><![CDATA[<p>对于带不等式约束的优化问题：</p>
<script type="math/tex; mode=display">\begin{aligned}
\min \limits_{\boldsymbol x} \quad f(\boldsymbol x) \\
s.t.  \quad g_i(\boldsymbol x) &\leq 0 \quad (i=1,...,n) \\
h_j(\boldsymbol x) &= 0 \quad (j=1,...,m)
\end{aligned}</script><p>若$f(\boldsymbol x),g_i(\boldsymbol x),h_j(\boldsymbol x)$具有连续的一阶偏导数，$\boldsymbol x^*$是优化问题的局部可行解（此时为极小值解），并且在$\boldsymbol x^*$处约束限制条件<sup><a href="#ref1">[1]</a></sup>成立，则存在$\boldsymbol \mu^*=(\mu_1^*,\mu_2^*,…,\mu_n^*)^T,\boldsymbol \lambda^*=(\lambda_1^*,\lambda_2^*,…,\lambda_m^*)^T,$使得：</p>
<script type="math/tex; mode=display">\left\{
\begin{aligned}
& \nabla L(\boldsymbol x^* ,\boldsymbol \mu^* ,\boldsymbol \lambda^* )=\nabla f(\boldsymbol  x^* )+\sum_{i=1}^{n}\mu_i^* \nabla g_i(\boldsymbol x^* )+\sum_{j=1}^{m}\lambda_j^* \nabla h_j(\boldsymbol x^*) &(1) \\
& h_j(\boldsymbol x^*)=0 &(2) \\
& g_i(\boldsymbol x^*) \leq 0 &(3) \\
& \mu_i^* \geq 0 &(4)\\
& \mu_i^* g_i(\boldsymbol x^*)=0 &(5)
\end{aligned}
\right.</script><p>其中$L(\boldsymbol x,\boldsymbol \mu,\boldsymbol \lambda)$为拉格朗日函数：</p>
<script type="math/tex; mode=display">L(\boldsymbol x,\boldsymbol \mu,\boldsymbol \lambda)=f(\boldsymbol x)+\sum_{i=1}^{n}\mu_i g_i(\boldsymbol x)+\sum_{j=1}^{m}\lambda_j h_j(\boldsymbol x)</script><p>以上即5条为KKT条件，其中条件(2)和(3)为约束条件显然成立，条件(1)、(4)、(5)成立的<strong>简单证明</strong>如下：</p>
<p><center>
<img src="./Inequality_constraint_diagram.png" width="60%" height="60%">
</center><br>由约束条件：$g_i(\boldsymbol x) \leq 0$可知，$\boldsymbol x^*$一定满足$g_i(\boldsymbol x^*) \lt 0$或者$g_i(\boldsymbol x^*) = 0$（也即条件(3)），对这两种情形进行讨论：</p>
<ul>
<li><strong>当$g_i(\boldsymbol x^*) \lt 0$时（如上左图）：</strong><br>此时极小值点在$g_i(\boldsymbol x) \lt 0$这个区域内，等价于<strong>无不等式约束条件</strong>的最优化问题，所以$\mu_i^* = 0$。</li>
<li><strong>当$g_i(\boldsymbol x^*) = 0$时（如上右图）：</strong><br>此时极小值点在$g_i(\boldsymbol x) = 0$这个边界上，等价于<strong>仅含等式约束条件</strong>的最优化问题，所以根据拉格朗日乘子法的原理可知，此时$f(\boldsymbol  x)$和$ g_i(\boldsymbol  x)=0$在极小值点处相切，梯度（正梯度）平行，即$\nabla f(\boldsymbol  x^*)+\mu_i^* \nabla g_i(\boldsymbol  x^*)=0 \quad (\mu_i^*∈\boldsymbol R)$。易知$g_i(\boldsymbol x)=0$这个边界上的梯度都是向外的，所以$\nabla f(\boldsymbol x^*)$必然和$\nabla g_i(\boldsymbol x^*)$的方向相反（如果不相反的话极小值点就不会在边界上取到而是在$g_i(\boldsymbol x) \lt 0$内取），所以$\nabla f(\boldsymbol  x^*)$和$\nabla g_i(\boldsymbol  x^*)$异号，也即$\mu_i^* \gt 0$。</li>
</ul>
<p>综上，条件(4)：$\mu_i^* \geq 0$恒成立；当$g_i(\boldsymbol x^*) \lt 0$时$\mu_i^* = 0$， $g_i(\boldsymbol x^*) = 0$时$\mu_i^* \gt 0$，所以条件(5)：$\mu_i^* g_i(\boldsymbol x^*)=0$恒成立；$\lambda_j$为拉格朗日乘子，由拉格朗日乘子法的原理可知，$\lambda_j$取值无要求，所以总存在$\lambda_j^*$使得条件(1)恒成立。<br>【注】：</p>
<ul>
<li>若局部可行解$\boldsymbol x^*$不满足约束限制条件：$LD^*=FD^*$，则<strong>不一定</strong>满足KKT条件；</li>
<li>KKT条件是局部可行解$\boldsymbol x^*$的一个<strong>必要条件</strong>；</li>
<li>对于带不等式约束的优化问题，若目标函数$f(\boldsymbol x)$是凸函数，不等式约束$g_i(\boldsymbol x)$是凸函数，等式约束$h_j(\boldsymbol x)$是仿射函数，则称该优化问题为<strong>凸优化</strong>，此时KKT条件升级为<strong>充要条件</strong>，$\boldsymbol x^*$升级为<strong>全局可行解</strong>；</li>
<li>以上所有论述的严格数学证明参见<a href="#ref1">[1]</a> § 4.2</li>
</ul>
<h3 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h3><p><span id="ref1">[1] 王燕军. 《最优化基础理论与方法》</span></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于带不等式约束的优化问题：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\min \limits_{\boldsymbol x} \quad f(\boldsymbol x) \\
s.t.  \qu
    
    </summary>
    
      <category term="最优化" scheme="http://www.sm1les.com/categories/%E6%9C%80%E4%BC%98%E5%8C%96/"/>
    
    
      <category term="KKT条件" scheme="http://www.sm1les.com/tags/KKT%E6%9D%A1%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归——三种似然项构造</title>
    <link href="http://www.sm1les.com/2018/12/04/logistic-regression-three-likelihood/"/>
    <id>http://www.sm1les.com/2018/12/04/logistic-regression-three-likelihood/</id>
    <published>2018-12-04T08:34:06.000Z</published>
    <updated>2018-12-11T05:45:43.365Z</updated>
    
    <content type="html"><![CDATA[<p>设</p>
<script type="math/tex; mode=display">p_1(\boldsymbol{x_i};\boldsymbol w)=\cfrac{e^{\boldsymbol w^T\boldsymbol x_i}}{1+e^{\boldsymbol w^T\boldsymbol x_i}},p_0(\boldsymbol{x_i};\boldsymbol w)=1-p_1(\boldsymbol{x_i};\boldsymbol w)=\cfrac{1}{1+e^{\boldsymbol w^T\boldsymbol x_i}}</script><h3 id="y-i-0-1-时（两种）"><a href="#y-i-0-1-时（两种）" class="headerlink" title="$y_i=0,1$时（两种）:"></a>$y_i=0,1$时（两种）:</h3><script type="math/tex; mode=display">p(y_i|\boldsymbol x_i;\boldsymbol w)=[p_1(\boldsymbol{x_i};\boldsymbol w)]^{y_i}[p_0(\boldsymbol{x_i};\boldsymbol w)]^{1-y_i}</script><script type="math/tex; mode=display">p(y_i|\boldsymbol x_i;\boldsymbol w)=y_ip_1(\boldsymbol{x_i};\boldsymbol w)+(1-y_i)p_0(\boldsymbol{x_i};\boldsymbol w)</script><p>对数似然函数的化简形式为：</p>
<script type="math/tex; mode=display">l(\boldsymbol w)=\sum_{i=1}^{m}(y_i\boldsymbol w^T\boldsymbol x_i-\ln(1+e^{\boldsymbol w^T\boldsymbol x_i}))</script><h3 id="y-i-pm-1-时（一种）"><a href="#y-i-pm-1-时（一种）" class="headerlink" title="$y_i=\pm 1$时（一种）:"></a>$y_i=\pm 1$时（一种）:</h3><script type="math/tex; mode=display">p(y_i|\boldsymbol x_i;\boldsymbol w)=\cfrac{1}{1+e^{-y_i\boldsymbol w^T\boldsymbol x_i}}</script><p>对数似然函数的化简形式为：</p>
<script type="math/tex; mode=display">l(\boldsymbol w)=-\sum_{i=1}^{m}\ln(1+e^{-y_i\boldsymbol w^T\boldsymbol x_i})</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;设&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_1(\boldsymbol{x_i};\boldsymbol w)=\cfrac{e^{\boldsymbol w^T\boldsymbol x_i}}{1+e^{\boldsymb
    
    </summary>
    
      <category term="机器学习" scheme="http://www.sm1les.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="逻辑回归" scheme="http://www.sm1les.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>线性回归——最（极）大似然估计和最小二乘法推导</title>
    <link href="http://www.sm1les.com/2018/11/27/linear-regression-maximum-likelihood/"/>
    <id>http://www.sm1les.com/2018/11/27/linear-regression-maximum-likelihood/</id>
    <published>2018-11-27T07:20:01.000Z</published>
    <updated>2018-12-03T09:57:23.598Z</updated>
    
    <content type="html"><![CDATA[<h3 id="最（极）大似然估计推导"><a href="#最（极）大似然估计推导" class="headerlink" title="最（极）大似然估计推导"></a>最（极）大似然估计推导</h3><p>对于线性回归模型：</p>
<script type="math/tex; mode=display">y=\boldsymbol{w}^T\boldsymbol{x}+b+\epsilon</script><p>随机误差$\epsilon$可以看成是由许多观察不到的、可加的微小误差叠加而成的，则根据中心极限定理，随机误差$\epsilon$服从正态分布$\epsilon \sim N(0,\sigma^2)$，接下来便可以用最（极）大似然估计法来估计$\boldsymbol{w}$和b的值。似然函数为：</p>
<script type="math/tex; mode=display">L(\boldsymbol{w},b)=L(\boldsymbol{x}_1,\cdots,\boldsymbol{x}_m;\boldsymbol{w},b)=\prod_{i=1}^{m}p(\epsilon_i)=\prod_{i=1}^{m}p(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)</script><p>又$p(\epsilon_i)=\cfrac{1}{\sqrt{2\pi}\sigma} \mathrm{exp}\left(-\cfrac{\epsilon_i^2}{2\sigma^2}\right)$，所以：</p>
<script type="math/tex; mode=display">L(\boldsymbol{w},b)=\prod_{i=1}^{m}\cfrac{1}{\sqrt{2\pi}\sigma} \mathrm{exp}\left(-\cfrac{(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2}{2\sigma^2}\right)</script><p>两边同时取对数得对数似然函数：</p>
<script type="math/tex; mode=display">
\begin{aligned}     
     \ln L(\boldsymbol{w},b) & = \sum_{i=1}^{m}\ln \cfrac{1}{\sqrt{2\pi}\sigma} \mathrm{exp}\left(-\cfrac{(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2}{2\sigma^2}\right) \\

     & = \sum_{i=1}^{m}\ln \cfrac{1}{\sqrt{2\pi}\sigma}+\sum_{i=1}^{m}\ln \mathrm{exp}\left(-\cfrac{(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2}{2\sigma^2}\right) \\

     & = m\ln \cfrac{1}{\sqrt{2\pi}\sigma}-\cfrac{1}{2\sigma^2}\sum_{i=1}^{m}(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2
\end{aligned}</script><p>其中$m$，$\sigma$均为常数，所以最大化$\ln L(\boldsymbol{w},b)$等价于最小化$\sum_{i=1}^{m}(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2$，也即：</p>
<script type="math/tex; mode=display">(\boldsymbol{w}',b')=\mathop{\arg\max}_{(\boldsymbol{w},b)} \ln L(\boldsymbol{w},b)=\mathop{\arg\min}_{(\boldsymbol{w},b)} \sum_{i=1}^{m}(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2</script><p>其中$\boldsymbol{w}’$和$b’$为$\boldsymbol{w}$和$b$的解.</p>
<h3 id="最小二乘法推导"><a href="#最小二乘法推导" class="headerlink" title="最小二乘法推导"></a>最小二乘法推导</h3><p>最小二乘法是基于均方误差最小化来对模型进行参数估计的，用公式表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned}    
    (\boldsymbol{w}',b') & = \mathop{\arg\min}_{(\boldsymbol{w},b)} \sum_{i=1}^{m}(y_i-(\boldsymbol{w}^T\boldsymbol{x_i}+b))^2 \\
    & = \mathop{\arg\min}_{(\boldsymbol{w},b)} \sum_{i=1}^{m}(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2
\end{aligned}</script><p>显然与最（极）大似然估计的推导结果一致。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;最（极）大似然估计推导&quot;&gt;&lt;a href=&quot;#最（极）大似然估计推导&quot; class=&quot;headerlink&quot; title=&quot;最（极）大似然估计推导&quot;&gt;&lt;/a&gt;最（极）大似然估计推导&lt;/h3&gt;&lt;p&gt;对于线性回归模型：&lt;/p&gt;
&lt;script type=&quot;ma
    
    </summary>
    
      <category term="机器学习" scheme="http://www.sm1les.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="线性回归" scheme="http://www.sm1les.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
</feed>
