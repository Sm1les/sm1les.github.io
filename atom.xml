<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Sm1les&#39;s blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.sm1les.com/"/>
  <updated>2018-12-04T09:06:07.156Z</updated>
  <id>http://www.sm1les.com/</id>
  
  <author>
    <name>Sm1les</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>逻辑回归——三种似然项构造</title>
    <link href="http://www.sm1les.com/2018/12/04/logistic-regression-three-likelihood/"/>
    <id>http://www.sm1les.com/2018/12/04/logistic-regression-three-likelihood/</id>
    <published>2018-12-04T08:34:06.000Z</published>
    <updated>2018-12-04T09:06:07.156Z</updated>
    
    <content type="html"><![CDATA[<p>设</p>
<script type="math/tex; mode=display">p_1(\boldsymbol{x_i};\boldsymbol w)=\cfrac{e^{\boldsymbol w^T\boldsymbol x_i}}{1+e^{\boldsymbol w^T\boldsymbol x_i}},p_0(\boldsymbol{x_i};\boldsymbol w)=1-p_1(\boldsymbol{x_i};\boldsymbol w)=\cfrac{1}{1+e^{\boldsymbol w^T\boldsymbol x_i}}</script><h3 id="y-i-0-1-时（两种）"><a href="#y-i-0-1-时（两种）" class="headerlink" title="$y_i=0,1$时（两种）:"></a>$y_i=0,1$时（两种）:</h3><script type="math/tex; mode=display">p(y_i|\boldsymbol x_i;\boldsymbol w)=[p_1(\boldsymbol{x_i};\boldsymbol w)]^{y_i}[p_0(\boldsymbol{x_i};\boldsymbol w)]^{1-y_i}</script><script type="math/tex; mode=display">p(y_i|\boldsymbol x_i;\boldsymbol w)=y_ip_1(\boldsymbol{x_i};\boldsymbol w)+(1-y_i)p_0(\boldsymbol{x_i};\boldsymbol w)</script><p>对数似然函数为：</p>
<script type="math/tex; mode=display">l(\boldsymbol w)=\sum_{i=1}^{m}(y_i\boldsymbol w^T\boldsymbol x_i-\ln(1+e^{\boldsymbol w^T\boldsymbol x_i}))</script><h3 id="y-i-pm-1-时（一种）"><a href="#y-i-pm-1-时（一种）" class="headerlink" title="$y_i=\pm 1$时（一种）:"></a>$y_i=\pm 1$时（一种）:</h3><script type="math/tex; mode=display">p(y_i|\boldsymbol x_i;\boldsymbol w)=\cfrac{1}{1+e^{-y_i\boldsymbol w^T\boldsymbol x_i}}</script><p>对数似然函数为：</p>
<script type="math/tex; mode=display">l(\boldsymbol w)=-\sum_{i=1}^{m}\ln(1+e^{-y_i\boldsymbol w^T\boldsymbol x_i})</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;设&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_1(\boldsymbol{x_i};\boldsymbol w)=\cfrac{e^{\boldsymbol w^T\boldsymbol x_i}}{1+e^{\boldsymb
    
    </summary>
    
      <category term="机器学习" scheme="http://www.sm1les.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="逻辑回归" scheme="http://www.sm1les.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>线性回归——最（极）大似然估计和最小二乘法推导</title>
    <link href="http://www.sm1les.com/2018/11/27/linear-regression-maximum-likelihood/"/>
    <id>http://www.sm1les.com/2018/11/27/linear-regression-maximum-likelihood/</id>
    <published>2018-11-27T07:20:01.000Z</published>
    <updated>2018-12-03T09:57:23.598Z</updated>
    
    <content type="html"><![CDATA[<h3 id="最（极）大似然估计推导"><a href="#最（极）大似然估计推导" class="headerlink" title="最（极）大似然估计推导"></a>最（极）大似然估计推导</h3><p>对于线性回归模型：</p>
<script type="math/tex; mode=display">y=\boldsymbol{w}^T\boldsymbol{x}+b+\epsilon</script><p>随机误差$\epsilon$可以看成是由许多观察不到的、可加的微小误差叠加而成的，则根据中心极限定理，随机误差$\epsilon$服从正态分布$\epsilon \sim N(0,\sigma^2)$，接下来便可以用最（极）大似然估计法来估计$\boldsymbol{w}$和b的值。似然函数为：</p>
<script type="math/tex; mode=display">L(\boldsymbol{w},b)=L(\boldsymbol{x}_1,\cdots,\boldsymbol{x}_m;\boldsymbol{w},b)=\prod_{i=1}^{m}p(\epsilon_i)=\prod_{i=1}^{m}p(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)</script><p>又$p(\epsilon_i)=\cfrac{1}{\sqrt{2\pi}\sigma} \mathrm{exp}\left(-\cfrac{\epsilon_i^2}{2\sigma^2}\right)$，所以：</p>
<script type="math/tex; mode=display">L(\boldsymbol{w},b)=\prod_{i=1}^{m}\cfrac{1}{\sqrt{2\pi}\sigma} \mathrm{exp}\left(-\cfrac{(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2}{2\sigma^2}\right)</script><p>两边同时取对数得对数似然函数：</p>
<script type="math/tex; mode=display">
\begin{aligned}     
     \ln L(\boldsymbol{w},b) & = \sum_{i=1}^{m}\ln \cfrac{1}{\sqrt{2\pi}\sigma} \mathrm{exp}\left(-\cfrac{(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2}{2\sigma^2}\right) \\

     & = \sum_{i=1}^{m}\ln \cfrac{1}{\sqrt{2\pi}\sigma}+\sum_{i=1}^{m}\ln \mathrm{exp}\left(-\cfrac{(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2}{2\sigma^2}\right) \\

     & = m\ln \cfrac{1}{\sqrt{2\pi}\sigma}-\cfrac{1}{2\sigma^2}\sum_{i=1}^{m}(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2
\end{aligned}</script><p>其中$m$，$\sigma$均为常数，所以最大化$\ln L(\boldsymbol{w},b)$等价于最小化$\sum_{i=1}^{m}(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2$，也即：</p>
<script type="math/tex; mode=display">(\boldsymbol{w}',b')=\mathop{\arg\max}_{(\boldsymbol{w},b)} \ln L(\boldsymbol{w},b)=\mathop{\arg\min}_{(\boldsymbol{w},b)} \sum_{i=1}^{m}(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2</script><p>其中$\boldsymbol{w}’$和$b’$为$\boldsymbol{w}$和$b$的解.</p>
<h3 id="最小二乘法推导"><a href="#最小二乘法推导" class="headerlink" title="最小二乘法推导"></a>最小二乘法推导</h3><p>最小二乘法是基于均方误差最小化来对模型进行参数估计的，用公式表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned}    
    (\boldsymbol{w}',b') & = \mathop{\arg\min}_{(\boldsymbol{w},b)} \sum_{i=1}^{m}(y_i-(\boldsymbol{w}^T\boldsymbol{x_i}+b))^2 \\
    & = \mathop{\arg\min}_{(\boldsymbol{w},b)} \sum_{i=1}^{m}(y_i-\boldsymbol{w}^T\boldsymbol{x_i}-b)^2
\end{aligned}</script><p>显然与最（极）大似然估计的推导结果一致。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;最（极）大似然估计推导&quot;&gt;&lt;a href=&quot;#最（极）大似然估计推导&quot; class=&quot;headerlink&quot; title=&quot;最（极）大似然估计推导&quot;&gt;&lt;/a&gt;最（极）大似然估计推导&lt;/h3&gt;&lt;p&gt;对于线性回归模型：&lt;/p&gt;
&lt;script type=&quot;ma
    
    </summary>
    
      <category term="机器学习" scheme="http://www.sm1les.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="线性回归" scheme="http://www.sm1les.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
</feed>
